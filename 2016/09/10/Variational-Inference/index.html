<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Variational Inference | K_Augus</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1. 背景在概率模型中，我们常常需要得到隐变量的后验分布或者计算相对于某个分布的期望，比如在EM算法中我们需要得到隐变量$Z$的后验分布，以及计算完全数据的似然分布相对于隐变量的后验分布的期望。然而对于很多现实中的模型，常常因为隐变量的维度过高，难以计算；或者期望太过复杂，没有闭式解。这时候我们就要寻求近似解。近似解大体上分为两种，一种是stochastic approximation，如MCMC">
<meta property="og:type" content="article">
<meta property="og:title" content="Variational Inference">
<meta property="og:url" content="http://yoursite.com/2016/09/10/Variational-Inference/index.html">
<meta property="og:site_name" content="K_Augus">
<meta property="og:description" content="1. 背景在概率模型中，我们常常需要得到隐变量的后验分布或者计算相对于某个分布的期望，比如在EM算法中我们需要得到隐变量$Z$的后验分布，以及计算完全数据的似然分布相对于隐变量的后验分布的期望。然而对于很多现实中的模型，常常因为隐变量的维度过高，难以计算；或者期望太过复杂，没有闭式解。这时候我们就要寻求近似解。近似解大体上分为两种，一种是stochastic approximation，如MCMC">
<meta property="og:image" content="http://yoursite.com/uploads/Variational_Inference.png">
<meta property="og:updated_time" content="2016-11-17T04:56:41.363Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Variational Inference">
<meta name="twitter:description" content="1. 背景在概率模型中，我们常常需要得到隐变量的后验分布或者计算相对于某个分布的期望，比如在EM算法中我们需要得到隐变量$Z$的后验分布，以及计算完全数据的似然分布相对于隐变量的后验分布的期望。然而对于很多现实中的模型，常常因为隐变量的维度过高，难以计算；或者期望太过复杂，没有闭式解。这时候我们就要寻求近似解。近似解大体上分为两种，一种是stochastic approximation，如MCMC">
<meta name="twitter:image" content="http://yoursite.com/uploads/Variational_Inference.png">
  
    <link rel="alternate" href="/atom.xml" title="K_Augus" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">K_Augus</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Variational-Inference" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/09/10/Variational-Inference/" class="article-date">
  <time datetime="2016-09-10T14:42:01.000Z" itemprop="datePublished">2016-09-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tutorial/">Tutorial</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Variational Inference
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2>1. 背景</h2><br>在概率模型中，我们常常需要得到隐变量的后验分布或者计算相对于某个分布的期望，比如在EM算法中我们需要得到隐变量$Z$的后验分布，以及计算完全数据的似然分布相对于隐变量的后验分布的期望。然而对于很多现实中的模型，常常因为隐变量的维度过高，难以计算；或者期望太过复杂，没有闭式解。这时候我们就要寻求近似解。近似解大体上分为两种，一种是stochastic approximation，如MCMC；另一种是deterministic approximation，比如我们这篇文章要讲的变分推断。<br><br>变分法最早来源于微积分，因为涉及到函数空间，所以叫变分。变分法的核心思想，就是从某个函数空间中找到满足某些条件或约束的函数。我们在统计推断中用到的变分法，实际上就是用形式简单的分布，去近似形式复杂、不易计算的分布，这样再做积分运算就会容易的多。<br><br><a id="more"></a><br><br><br>———-<br><br><h2>2. 基础推导</h2><br>跟EM里的推导一样，似然函数可以推导成一个下界加上一个相对熵的形式：<br>\begin{align}<br>ln(p(X)) &amp;= ln\left(\frac{p(X,Z)}{q(Z)}\right) - ln\left(\frac{p(Z|X)}{q(Z)}\right) \\<br>&amp;=\int q(Z)ln\left(\frac{p(X,Z)}{q(Z)}\right)dZ - \int q(Z)ln\left(\frac{p(Z|X)}{q(Z)}\right)dZ \\<br>&amp;=\underbrace{\int q(Z)ln(p(X,Z))dZ - \int q(Z)ln(q(Z))dZ}_{\mathcal{L}(q)} + \underbrace{\left(-\int q(Z)ln\left(\frac{p(Z|X)}{q(Z)}\right)dZ\right)}_{KL(q||p)} \\<br>&amp;= \mathcal{L}(q) + KL(q||p)<br>\end{align}<br>这里的形式跟EM不同的是参数$\Theta$也包含到了随机变量$Z$里。$\mathcal{L}(q)$叫做Evidence Lower Bound(ELOB)，因为相对熵是恒不小于0的。$\mathcal{L}$是关于函数$q(Z)$的泛函【functional】<br>不同于EM，这里$p(Z|X)$是intractable的，所以在最小化KL divergence的时候，我们需要限制$q(Z)$可选的分布类型——既要tractable，也能提供一个好的approximation。而且，这里不会有”over-fitting”，因为越逼近真实的后验分布越好<br><br>———-<br><br><h2>3. Mean field</h2><br>常用的是限制$q(Z)$为可分解的分布【factorized distributions】，将$Z$分解为$M$组变量$Z_i$，即<br>\begin{equation}<br>q(Z) = \prod_{i=1}^Mq_i(Z_i)<br>\end{equation}<br>这个分解通常与模型相关。在物理上，这种形式的变分推断被称为mean filed theory。<br>将上式的分解代入到$\mathcal{L}(q)$，为了让表达更加简洁明了，用$q_i$表示$q_i(Z_i)$：<br>\begin{align}<br>\mathcal{L}(q) &amp;= \int q(Z)ln(p(X,Z))dZ - \int q(Z)ln(q(Z))dZ \\<br>&amp;= \underbrace{\int \prod_{i=1}^Mq_iln(p(X,Z))dZ}_{part (1)} - \underbrace{\int \prod_{i=1}^Mq_i\sum_{i=1}^Mlnq_idZ}_{part (2)}<br>\end{align}<br><br>\begin{align}<br>(Part\ 1) &amp;= \int \prod_{i=1}^Mq_iln(p(X,Z))dZ \\<br>&amp;= \int_{Z_1}\ldots\int_{Z_M}\prod_{i=1}^Mq_iln(p(X,Z))dZ_1,…,dZ_M \\<br>&amp;= \int_{Z_j}q_j\left(\int_{Z_{i\neq j}}\prod_{i\neq j}^Mq_iln(p(X,Z))\prod_{i\neq j}^MdZ_i\right)dZ_j \\<br>&amp;= \int_{Z_j}q_j\left(\int_{Z_{i\neq j}}ln(p(X,Z))\prod_{i\neq j}^Mq_idZ_i\right)dZ_j \\<br>&amp;= \int_{Z_j}q_j(Z_j)[E_{i\neq j}[ln(p(X,Z))]]dZ_j<br>\end{align}<br><br>\begin{align}<br>(Part\ 2) &amp;= \int \prod_{i=1}^Mq_i\sum_{i=1}^Mln(q_i)dZ \\<br>&amp;= \sum_{j=1}^M\left(\int_{Z_j}q_jln(q_j)\left(\int_{Z_{i\neq j}}\prod_{Z_{i\neq j}}q_idZ_{i\neq j}\right)dZ_j\right) \\<br>&amp;= \sum_{j=1}^M\int_{Z_j}q_jln(q_j)dZ_j<br>\end{align}<br>所以，对于某个特定的$q_j$：<br>\begin{align}<br>\mathcal{L}(q) &amp;= \int_{Z_j}q_j\underbrace{[E_{i\neq j}[ln(p(X,Z))]]}_{ln(\tilde{p}_j(X,Z_j))}dZ_j - \int_{Z_j}q_jln(q_j)dZ_j + \underbrace{const}_{terms\ not\ involve\ q_j} \\<br>&amp;= \int_{Z_j}q_jln\frac{ln(\tilde{p}_j(X,Z_j))}{q_j} + const<br>\end{align}<br>这也是一个负的KL divergence，所以我们可以通过最小化这个KL divergence来最大化$\mathcal{L}(q)$，这时最优的$q^*_j$满足<br>\begin{equation}<br>ln(q_j^*) = E_{i\neq j}[ln(p(X,Z))]<br>\end{equation}<br>这条式子的意思是：因子$q_j$最优解的log为完全数据【观测变量和隐变量】的log联合分布相对于其他因子$q_i,  i\neq j$的期望——这是变分推断的基础。通常我们不需要考虑const那一项，因为const项就是归一化项，归一化项通常可以通过观察得到。<br>我们用坐标下降的方法迭代更新每个因子直到收敛。因为下界对于每个因子都是凸的，所以这个过程保证收敛。<br><br><br>———-<br><h2>4. $KL(p||q)\ vs \ KL(q||p)$</h2><br>上面用的优化是$KL(q||p)$，现在考虑一般情况下用可分解的$q(Z)$最小化$KL(p||q)$的问题：<br>\begin{equation}<br>KL(p||q)=-\int p(Z)\left[\sum_{i=1}^Mln(q_i)\right]dZ+\underbrace{\int p(Z)lnp(Z)dZ}_{const}<br>\end{equation}<br>像上面一样只考虑对某个因子$q_j$做优化，则<br>\begin{align}<br>KL(p||q) &amp;= -\int p(Z)\left[\sum_{i=1}^Mln(q_i)\right]dZ+const \\<br>&amp;= -\int\left(p(Z)ln(q_j)+p(Z)\sum_{i\neq j}ln(q_i)\right)dZ + const \\<br>&amp;= -\int p(Z)ln(q_j)dZ + const \\<br>&amp;= -\int ln(q_j)\underbrace{\left[\int p(Z)\prod_{i\neq j}dZ_i\right]}_{F_j(Z_j)}dZ_j + const \\<br>&amp;= -\int F_j(Z_j)ln(q_j)dZ_j + const<br>\end{align}<br>用拉格朗日乘子法约束$q_j$为一个分布：<br>\begin{equation}<br>-\int F_j(Z_j)ln(q_j)dZ_j + \lambda\left(\int q_jdZ_j -1\right)<br>\end{equation}<br>用变分法的欧拉-拉格朗日方程求解可以得到<br>\begin{equation}<br>-\frac{F_j(Z_j)}{q_j}+\lambda = 0<br>\end{equation}<br>也即<br>\begin{equation}<br>\lambda q_j = F_j(Z_j)<br>\end{equation}<br>两边对$Z_j$积分，可得<br>\begin{equation}<br>\lambda=\int F_j(Z_j)dZ_j=1<br>\end{equation}<br>所以<br>\begin{equation}<br>q_j = F_j(Z_j) = \int p(Z)\prod_{i\neq j}dZ_i = p(Z_j)<br>\end{equation}<br>就是说，在优化$KL(p||q)$的情况下，因子$q_j$的最优解有刚好就是相应的边缘分布$p(Z_j)$<br>PRML里的一幅图描述了对二元高斯分布分别用$KL(q||p)$和$KL(p||q)$优化的结果<br><img src="/uploads/Variational_Inference.png" alt="Variational Inference"><br>左边是$KL(q||p)$，右边是$KL(p||q)$，两种情况下都能很好得拟合均值，然而对于整体分布的拟合情况确有很大差别。这可以从KL divergence的式子里直接考虑<br>\begin{equation}<br>KL(q||p)=-\int q(Z)ln\frac{p(Z)}{q(Z)}dZ<br>\end{equation}<br>这里对值影响比较大的部分主要来自$ln$相除的那部分。对于$KL(q||p)$，在$p(Z)$比较小的地方$q(Z)$也得比较小，不然的话一除，再ln，这个值就会非常大，这叫”zero forcing”；相反，对于$KL(p||q)$的情况，在$p(Z)$比较大的地方，$q(Z)$也得比较大，这种情况叫”zero avoiding”。这就造成了上图左边只在高密度区域有值，而右边则是整体上都有值的结果。<br><br><br>———-<br><h2>5. 极大似然、EM与变分</h2>


<hr>
<h2>6. 指数分布族与变分</h2>


<hr>
<h2>7. Expectation Propagation</h2>


<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/09/10/Variational-Inference/" data-id="civn67rao000iy0cvcsyy2pm5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Bayesian/">Bayesian</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tutorial/">Tutorial</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/09/11/Distilling-the-Knowledge-in-a-Neural-Network/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Distilling the Knowledge in a Neural Network
        
      </div>
    </a>
  
  
    <a href="/2016/09/04/Self-Paced-Learning-with-Diversity-NIPS14/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Self-Paced Learning with Diversity_NIPS14</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/ModelCompression/">ModelCompression</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/PaperNotes/">PaperNotes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SelfPacedLearning/">SelfPacedLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/TransferLearning/">TransferLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tutorial/">Tutorial</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayesian/">Bayesian</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bilinear/">Bilinear</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DL/">DL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DepthMap/">DepthMap</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EM/">EM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gaussian/">Gaussian</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/">ICLR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/">ICML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Latent/">Latent</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ModelCompression/">ModelCompression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/">NIPS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SelfPacedLearning/">SelfPacedLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transfer/">Transfer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tutorial/">Tutorial</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Bayesian/" style="font-size: 15px;">Bayesian</a> <a href="/tags/Bilinear/" style="font-size: 10px;">Bilinear</a> <a href="/tags/DL/" style="font-size: 10px;">DL</a> <a href="/tags/DepthMap/" style="font-size: 10px;">DepthMap</a> <a href="/tags/EM/" style="font-size: 10px;">EM</a> <a href="/tags/Gaussian/" style="font-size: 10px;">Gaussian</a> <a href="/tags/ICLR/" style="font-size: 10px;">ICLR</a> <a href="/tags/ICML/" style="font-size: 10px;">ICML</a> <a href="/tags/Latent/" style="font-size: 15px;">Latent</a> <a href="/tags/ModelCompression/" style="font-size: 10px;">ModelCompression</a> <a href="/tags/NIPS/" style="font-size: 20px;">NIPS</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/SelfPacedLearning/" style="font-size: 15px;">SelfPacedLearning</a> <a href="/tags/Transfer/" style="font-size: 20px;">Transfer</a> <a href="/tags/Tutorial/" style="font-size: 20px;">Tutorial</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/11/25/Unsupervised-Learning-PaperNotes/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/11/18/Auto-Encoding-Variational-Bayes-iclr14/">Auto-Encoding Variational Bayes_iclr14</a>
          </li>
        
          <li>
            <a href="/2016/09/17/Depth-Map-Prediction/">Depth Map Prediction</a>
          </li>
        
          <li>
            <a href="/2016/09/15/Gaussian-distribution/">Gaussian distribution</a>
          </li>
        
          <li>
            <a href="/2016/09/11/Distilling-the-Knowledge-in-a-Neural-Network/">Distilling the Knowledge in a Neural Network</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 K_Augus<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>