<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Gaussian distribution | K_Augus</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="这篇文章主要是讲一些常用的高斯分布的性质。那么，为什么高斯分布这么常用呢？我觉得主要有两个原因，一个是中心极限定理；第二个则是因为用高斯分布往往能得到漂亮的闭式解。">
<meta property="og:type" content="article">
<meta property="og:title" content="Gaussian distribution">
<meta property="og:url" content="http://yoursite.com/2016/09/15/Gaussian-distribution/index.html">
<meta property="og:site_name" content="K_Augus">
<meta property="og:description" content="这篇文章主要是讲一些常用的高斯分布的性质。那么，为什么高斯分布这么常用呢？我觉得主要有两个原因，一个是中心极限定理；第二个则是因为用高斯分布往往能得到漂亮的闭式解。">
<meta property="og:updated_time" content="2016-09-15T14:08:58.148Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Gaussian distribution">
<meta name="twitter:description" content="这篇文章主要是讲一些常用的高斯分布的性质。那么，为什么高斯分布这么常用呢？我觉得主要有两个原因，一个是中心极限定理；第二个则是因为用高斯分布往往能得到漂亮的闭式解。">
  
    <link rel="alternate" href="/atom.xml" title="K_Augus" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">K_Augus</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Zoeken"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Gaussian-distribution" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/09/15/Gaussian-distribution/" class="article-date">
  <time datetime="2016-09-15T14:08:26.000Z" itemprop="datePublished">2016-09-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tutorial/">Tutorial</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Gaussian distribution
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这篇文章主要是讲一些常用的高斯分布的性质。那么，为什么高斯分布这么常用呢？我觉得主要有两个原因，一个是中心极限定理；第二个则是因为用高斯分布往往能得到漂亮的闭式解。</p>
<a id="more"></a>
<p>首先，单元和多元的高斯分布分别如下：<br>\begin{equation}<br>\mathcal{N}(x|\mu, \sigma^2)=\frac{1}{(2\pi\sigma^2)^{1/2}}exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}<br>\end{equation}<br>\begin{equation}<br>\mathcal{N}(\mathbf{x}|\mathbf{\mu}, \Sigma)=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\left\{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})\right\}<br>\end{equation}</p>
<p><strong>1. 在support为$[-\infty,\infty]$以及给定均值和方差的情况下，高斯分布是熵最大的分布</strong><br>首先，熵的定义为<br>\begin{equation}<br>H(x) = -\int p(x)lnp(x)dx<br>\end{equation}<br>给定方差的情况下，$p(x)$需要满足以下几个约束<br>\begin{equation}<br>\int_{-\infty}^{\infty}p(x)dx = 1<br>\end{equation}<br>\begin{equation}<br>\int_{-\infty}^{\infty}xp(x)dx = \mu<br>\end{equation}<br>\begin{equation}<br>\int_{-\infty}^{\infty}(x-\mu)^2p(x)dx = \sigma^2<br>\end{equation}<br>用拉格朗日乘子法，我们需要最大化下面的这个函数<br>\begin{equation}<br>-\int_{-\infty}^{\infty}p(x)lnp(x)dx+\lambda_1\left(\int_{-\infty}^{\infty}p(x)dx-1\right)+\\<br>\lambda_2\left(\int_{-\infty}^{\infty}xp(x)dx - \mu\right)+\lambda_3\left(\int_{-\infty}^{\infty}(x-\mu)^2p(x)dx- \sigma^2\right)<br>\end{equation}<br>用变分法求解【对函数p(x)求导】，可得<br>\begin{equation}<br>p(x)=exp\{-1+\lambda_1+\lambda_2x+\lambda_3(x-\mu)^2\}<br>\end{equation}<br>为了让$\int_Rp(x)dx$有限，所以$\lambda_2$应该等于0，$\lambda_3$应该小于0。这样子，剩下的$\lambda_1$和$\lambda_3$可以通过观察得到<br>\begin{align}<br>\lambda_1 &amp;= 1 - \frac{1}{2}ln(2\pi\sigma^2)\\<br>\lambda_2 &amp;= 0\\<br>\lambda_3 &amp;= -\frac{1}{2\sigma^2}<br>\end{align}<br>所以<br>\begin{equation}<br>p(x)=\frac{1}{(2\pi\sigma^2)^{1/2}}exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}<br>\end{equation}<br>多元的推导需要更多的trick，这里暂时不写</p>
<p><strong>2. 变量的联合分布是高斯，则变量的条件分布和边缘分布也是高斯</strong><br>将变量、均值、协方差矩阵和精度矩阵【precision matrix，协方差矩阵的逆】分块：<br>\begin{equation}<br>x=\begin{bmatrix}<br>    x_a \\<br>    x_b<br>\end{bmatrix},\quad<br>\mu=\begin{bmatrix}<br>    \mu_a \\<br>    \mu_b<br>\end{bmatrix},\quad<br>\Sigma=\begin{bmatrix}<br>    \Sigma_{aa} &amp; \Sigma_{ab}\\<br>    \Sigma_{ba} &amp; \Sigma_{bb}<br>\end{bmatrix},\quad<br>\Lambda=\begin{bmatrix}<br>    \Lambda_{aa} &amp; \Lambda_{ab}\\<br>    \Lambda_{ba} &amp; \Lambda_{bb}<br>\end{bmatrix}<br>\end{equation}<br>这里用精度矩阵只是为了表示上的方便。注意精度矩阵也是对称矩阵【对称矩阵的逆也是对称矩阵】。<br>为了得到$p(x_a|x_b)$和$p(x_a)$，通常采用的方法是配方法【complete the square】。因为高斯分布的的指数可以写成<br>\begin{equation}<br>-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)=\frac{1}{2}x^T\Sigma^{-1}x+x^T\Sigma^{-1}\mu+const<br>\end{equation}<br>所以对于一般的二次型，如果可以写成上面这样的形式，那么我们就可以直接得到我们想要的均值和协方差。<br>我们只讨论$p(x_a|x_b)$，对于$p(x_b|x_a)$的情况，其实是对称的。考虑联合分布$p(x_a,x_b)$指数项里的二次型：<br>\begin{align}<br>\begin{split}<br>&amp;-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) = \\<br>&amp;\quad-\frac{1}{2}(x_a-\mu_a)^T\Lambda_{aa}(x_a-\mu_a) -\frac{1}{2}(x_a-\mu_a)^T\Lambda_{ab}(x_b-\mu_b) \\<br>&amp;\quad-\frac{1}{2}(x_b-\mu_b)^T\Lambda_{ba}(x_a-\mu_a) -\frac{1}{2}(x_b-\mu_b)^T\Lambda_{bb}(x_b-\mu_b)<br>\end{split}<br>\end{align}<br>提取出里面$x_a$的二次项：$-\frac{1}{2}x_a^T\Lambda_{aa}x_a$，所以我们可以直接推断出<br>\begin{equation}<br>\Sigma_{a|b}=\Lambda_{aa}^{-1}<br>\end{equation}<br>现在考虑$x_a$的一次项：<br>\begin{equation}<br>x_a^T\{\Lambda_{aa}\mu_a-\Lambda_{ab}(x_b-\mu_b)\} = x_a^T\Sigma_{a|b}^{-1}\mu_{a|b}<br>\end{equation}<br>所以<br>\begin{align}<br>\mu_{a|b} &amp;= \Sigma_{a|b}\{\Lambda_{aa}\mu_a-\Lambda_{ab}(x_b-\mu_b)\} \\<br>&amp;= \mu_a - \Lambda_{aa}^{-1}\Lambda_{ab}(x_b-\mu_b)<br>\end{align}<br>这里是用分块的精度矩阵来表示的，为了用原来的协方差矩阵来表示，我们需要用到分块矩阵的逆的表达式，即：<br>\begin{equation}<br>\begin{bmatrix}<br>    A &amp; B \\<br>    C &amp; D<br>\end{bmatrix}^{-1}=<br>\begin{bmatrix}<br>    M &amp; -MBD^{-1} \\<br>    -D^{-1}CM &amp; D^{-1}+D^{-1}CMBD^{-1}<br>\end{bmatrix}<br>\end{equation}<br>这里$M=(A-BD^{-1}C)^{-1})$，$M^{-1}$被叫做舒尔补【Schur Complement】。<br>所以<br>\begin{align}<br>\Lambda_{aa} &amp;= (\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1} \\<br>\Lambda_{ab} &amp;= -(\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\Sigma_{ab}\Sigma_{bb}^{-1}<br>\end{align}<br>把上面两式代替到$\mu_{a|b}$和$\Sigma_{a|b}$的表达式中，即可得到用分块的协方差矩阵的表达形式：<br>\begin{align}<br>\mu_{a|b} &amp;= \mu_a + \Sigma_{ab}\Sigma_{bb}^{-1}(x_b-\mu_b)\\<br>\Sigma_{a|b} &amp;= \Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}<br>\end{align}<br>从信息熵的角度来讲，给定$b$的条件下，$a$的不确定性减少了，减少的量与$a,b$之间的协方差的平方成正比——也就是说，$a$和$b$越相关，给定$b$的条件下，$a$的方差越小。<br>注意到$\mu_{a|b}$是$x_b$的线性函数，而$\Sigma_{a|b}$与$x_a$无关。这实际上是线性高斯模型的例子。</p>
<p>接下来考虑边缘分布:$p(x_b)=\int p(x_a,x_b)dx_a$。要把$x_a$积分积掉，那么我们还是先把跟$x_a$相关的项先挑出来，配方，这在上面其实已经做过了，这回把它完整得写出来<br>\begin{equation}<br>-\frac{1}{2}x_a^T\Lambda_{aa}x_a+x_a^T\Lambda_{aa}\mu_{a|b}=-\frac{1}{2}(x_a-\mu_{a|b})^T\Lambda_{aa}(x_a-\mu_{a|b})+\frac{1}{2}\mu_{a|b}^T\Lambda_{aa}\mu_{a|b}<br>\end{equation}<br>注意到$\int exp\{-\frac{1}{2}(x_a-\mu_{a|b})^T\Lambda_{aa}(x_a-\mu_{a|b})\}dx_a$积分得到的其实就是$p(x_a|x_b)$的归一化项的倒数，这是一个常数。这样我们就能把$x_a$积分积掉了。上式中剩下的<br>\begin{align}<br>&amp;\quad \frac{1}{2}\mu_{a|b}^T\Lambda_{aa}\mu_{a|b} \\<br>&amp;= \frac{1}{2}(\mu_a - \Lambda_{aa}^{-1}\Lambda_{ab}(x_b-\mu_b))^T\Lambda_{aa}(\mu_a - \Lambda_{aa}^{-1}\Lambda_{ab}(x_b-\mu_b)) \\<br>&amp;=\frac{1}{2}x_b^T\Lambda_{ba}\Lambda_{aa}^{-1}\Lambda_{ab} -x_b^T\Lambda_{ab}\mu_a - x_b^T\Lambda_{ba}\Lambda_{aa}^{-1}\Lambda_{ab}\mu_a + const<br>\end{align}<br>所以式子(15)中剩下的所有项：<br>\begin{align}<br>&amp;\quad -\frac{1}{2}x_b^T\Lambda_{bb}x_b + x_b^T(\Lambda_{bb}\mu_b+\Lambda_{ba}\mu_a)+const \\<br>&amp;= - \frac{1}{2}x_b^T(\Lambda_{bb} - \Lambda_{ba}\Lambda_{aa}^{-1}\Lambda_{ab})x_b + x_b^T(\Lambda_{bb}- \Lambda_{ba}\Lambda_{aa}^{-1}\Lambda_{ab})u_b + const<br>\end{align}<br>根据式子(14)和(21)，我们很容易得到$p(x_b)$的边缘分布的均值和方差分别就是$\mu_b$和$\Sigma_{bb}$</p>
<p><strong>3. 已知$p(x)$和$p(y|x)$，求$p(y)$和$p(x|y)$</strong><br>假设<br>\begin{align}<br>p(x) &amp;= \mathcal{N}(x|\mu, \Lambda^{-1}) \\<br>p(y|x) &amp;= \mathcal{N}(y|Ax+b,L^{-1})<br>\end{align}<br>注意这是上面所说的线性高斯模型，即$y$的均值与$x$线性相关，而$y$的方差与$x$无关。<br>考虑$x,y$的log联合分布，首先定义$z=[x,y]^T$，则<br>\begin{align}<br>lnp(z) &amp;= lnp(x)+lnp(y|x) \\<br>&amp;= -\frac{1}{2}(x-\mu)^T\Lambda(x-\mu) + \\<br>&amp;\quad -\frac{1}{2}(y-Ax-b)^TL(y-Ax-b)+const<br>\end{align}<br>和第2节一样，注意到这其实是$z$的二次函数，我们可以对上式进行配方，得到$p(z)$的表达式。首先考虑二次项<br>\begin{equation}<br>-\frac{1}{2}(x^T(\Lambda+A^TLA)x + y^TLy - y^TLAx - x^TA^TLy \\<br>= \frac{1}{2}[x, y]<br>\begin{bmatrix}<br>\Lambda+A^TLA &amp; -A^TL \\<br>-LA &amp; L<br>\end{bmatrix}<br>[x,y]^T = \frac{1}{2}z^TR^z<br>\end{equation}<br>也即$z$的高斯分布的精度矩阵为<br>\begin{equation}<br>R=\begin{bmatrix}<br>\Lambda+A^TLA &amp; -A^TL \\<br>-LA &amp; L<br>\end{bmatrix}<br>\end{equation}<br>取逆【按公式(20)直接算】可以得到协方差矩阵<br>\begin{equation}<br>cov[z]=\begin{bmatrix}<br>\Lambda^{-1} &amp; \Lambda^{-1}A^T \\<br>A\Lambda^{-1} &amp; L^{-1}+A\Lambda^{-1}A^T<br>\end{bmatrix}<br>\end{equation}<br>接下来取出一次项<br>\begin{equation}<br>x^T\Lambda\mu-x^TA^TLb+y^TLb=[x, y][\Lambda\mu-A^TLb, Lb]^T<br>\end{equation}<br>根据(14)<br>\begin{equation}<br>E[z]=[\mu, A\mu+b]^T<br>\end{equation}<br>注意到这分别就是对应的$x$和$y$的均值组成的向量</p>
<p>利用从第2节推导的边缘分布的结果，我们可以轻松从联合变量$z$的分布中读出<br>\begin{align}<br>E[y] &amp;= A\mu+b \\<br>cov[y] &amp;= L^{-1}+A\Lambda^{-1}A^T<br>\end{align}<br>注意到这里如果$A=I$，那么$cov[y]$的结果其实就是$cov[y|x]$加上$cov[x]$<br>根据式子(16)、(19)，我们也可以轻松得得到$p(x|y)$的结果<br>\begin{align}<br>E[x|y] &amp;= E[x] - R_{xx}^{-1}R_{xy}(y-E[y]) \\<br>&amp;= \mu - (\Lambda+A^TLA)^{-1}(-A^TL)(y-(A\mu+b)) \\<br>&amp;= (\Lambda+A^TLA)^{-1}(A^TL(y-b)+\Lambda\mu) \\<br>cov[x|y] &amp;= R_{xx}^{-1} = (\Lambda+A^TLA)^{-1}<br>\end{align}<br>这些推导结果在贝叶斯的推理过程中会很有用。</p>
<p><strong>4. 先验【挖坑】</strong><br>(1) 已知方差，均值未知的情况下，高斯分布的先验分布仍然是高斯分布<br>(2) 已知均值，方差未知的情况下，单变量高斯分布的先验分布是inverse gamma分布；多变量高斯分布的先验分布是inverse Wishart分布<br>(3) 均值和方差均未知的情况下，单变量高斯分布的先验分布是inverse normal/gaussian-gamma分布；多变量高斯分布的先验分布是inverse  normal/gaussian-Wishart分布</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/09/15/Gaussian-distribution/" data-id="civn67rac0009y0cvdviaeoat" class="article-share-link">Delen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Gaussian/">Gaussian</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tutorial/">Tutorial</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/09/17/Depth-Map-Prediction/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Nieuwer</strong>
      <div class="article-nav-title">
        
          Depth Map Prediction
        
      </div>
    </a>
  
  
    <a href="/2016/09/11/Distilling-the-Knowledge-in-a-Neural-Network/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Ouder</strong>
      <div class="article-nav-title">Distilling the Knowledge in a Neural Network</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categorieën</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/ModelCompression/">ModelCompression</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/PaperNotes/">PaperNotes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SelfPacedLearning/">SelfPacedLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/TransferLearning/">TransferLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tutorial/">Tutorial</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Labels</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayesian/">Bayesian</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bilinear/">Bilinear</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DL/">DL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DepthMap/">DepthMap</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EM/">EM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gaussian/">Gaussian</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICLR/">ICLR</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICML/">ICML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Latent/">Latent</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ModelCompression/">ModelCompression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIPS/">NIPS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SelfPacedLearning/">SelfPacedLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transfer/">Transfer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tutorial/">Tutorial</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Bayesian/" style="font-size: 15px;">Bayesian</a> <a href="/tags/Bilinear/" style="font-size: 10px;">Bilinear</a> <a href="/tags/DL/" style="font-size: 10px;">DL</a> <a href="/tags/DepthMap/" style="font-size: 10px;">DepthMap</a> <a href="/tags/EM/" style="font-size: 10px;">EM</a> <a href="/tags/Gaussian/" style="font-size: 10px;">Gaussian</a> <a href="/tags/ICLR/" style="font-size: 10px;">ICLR</a> <a href="/tags/ICML/" style="font-size: 10px;">ICML</a> <a href="/tags/Latent/" style="font-size: 15px;">Latent</a> <a href="/tags/ModelCompression/" style="font-size: 10px;">ModelCompression</a> <a href="/tags/NIPS/" style="font-size: 20px;">NIPS</a> <a href="/tags/SVM/" style="font-size: 15px;">SVM</a> <a href="/tags/SelfPacedLearning/" style="font-size: 15px;">SelfPacedLearning</a> <a href="/tags/Transfer/" style="font-size: 20px;">Transfer</a> <a href="/tags/Tutorial/" style="font-size: 20px;">Tutorial</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archieven</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recente berichten</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/11/18/Auto-Encoding-Variational-Bayes-iclr14/">Auto-Encoding Variational Bayes_iclr14</a>
          </li>
        
          <li>
            <a href="/2016/09/17/Depth-Map-Prediction/">Depth Map Prediction</a>
          </li>
        
          <li>
            <a href="/2016/09/15/Gaussian-distribution/">Gaussian distribution</a>
          </li>
        
          <li>
            <a href="/2016/09/11/Distilling-the-Knowledge-in-a-Neural-Network/">Distilling the Knowledge in a Neural Network</a>
          </li>
        
          <li>
            <a href="/2016/09/10/Variational-Inference/">Variational Inference</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 K_Augus<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>