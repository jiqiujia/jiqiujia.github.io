<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>K_Augus</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-11-18T02:16:18.473Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>K_Augus</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Auto-Encoding Variational Bayes_iclr14</title>
    <link href="http://yoursite.com/2016/11/18/Auto-Encoding-Variational-Bayes-iclr14/"/>
    <id>http://yoursite.com/2016/11/18/Auto-Encoding-Variational-Bayes-iclr14/</id>
    <published>2016-11-18T02:16:18.000Z</published>
    <updated>2016-11-18T02:16:18.473Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Depth Map Prediction</title>
    <link href="http://yoursite.com/2016/09/17/Depth-Map-Prediction/"/>
    <id>http://yoursite.com/2016/09/17/Depth-Map-Prediction/</id>
    <published>2016-09-17T04:18:56.000Z</published>
    <updated>2016-09-17T07:09:26.024Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要是一些做深度图像预测的论文笔记。</p>
<a id="more"></a>
<p></p><h2>1. Depth Map Prediction from a Single Image using a Multi-Scale Deep Network_NIPS14</h2><br>这篇文章其实像是一个coarse to fine的cascade模型，直接先看网络模型吧<br><img src="https://cl.ly/3P261n1t023f/Depth%20Map%20Prediction%20from%20a%20Single%20Image.png" alt="Depth Map Prediction from a Single Image using a Multi-Scale Deep Network"><br>就是把第一层网络的coarse输出拼接到第二层的网络上去。<br>然后作者提出一种缩放尺度不变的mean square error做回归<br>\begin{align}<br>D(y,y^*) &amp;= \frac{1}{2n^2}\sum_{i,j}((logy_i-logy_j)-(logy_i^*-logy_j^*))^2 \\<br>&amp;= \frac{1}{n}\sum_id_i^2-\frac{1}{n^2}\sum_{i,j}d_id_j\\<br>&amp;= \frac{1}{n}\sum_id_i^2-\frac{1}{n^2}\left(\sum_id_i\right)^2<br>\end{align}<br>其中，$y$是预测的值，$y^*$是ground truth，$d_i=logy_i-logy_i^*$<br>这样子，为了得到小的loss，预测的深度图像中每一对像素值之差应该与ground truth之间的差不多。然而个人觉得，这个loss其实会造成整体的灰度与ground truth之间有一定的差别，从作者给出的训练结果可以看出这一点<br><img src="https://cl.ly/0N3S3S373H44/Depth%20Map%20Prediction%20from%20a%20Single%20Image_2.png" alt="Depth Map Prediction from a Single Image using a Multi-Scale Deep Network_2"><br>(a)是原图，(b)是第一层网络的coarse输出，(c)是第二层网络的fine输出，(d)是ground truth<p></p>
<p></p><h2>2. Deep Convolutional Neural Fields for Depth Estimation from a Single Image_CVPR15</h2><br>还是先看网络架构<br><img src="https://cl.ly/3X3T1a3G3X1o/Deep%20Convolutional%20Neural%20Fields%20for%20Depth%20Estimation%20from%20a%20Single%20Image_1.png" alt="Deep Convolutional Neural Fields for Depth Estimation from a Single Image_1"><br>作者先把图片用SLIC算法分割成超像素，对于每个超像素，以那个超像素的中心为中心取一个224*224的块，输入到深度网络中得到预测值$z_p$，算回归loss，$\sum U(y_p,x)=\sum(y_p-z_p)^2$；对于相邻的超像素块，还会计算它们的$K$种相似度【文中用了3种：颜色差，颜色直方图差和LBP特征，$S_{pq}^{(k)}=e^{-\gamma|s_p^{(k)}-s_q^{(k)}}|$】，这些特征通过一个全连接层，得到$R_{pq}$，算pairwise loss$\sum V(y_p,y_q,x)=\sum\frac{1}{2}R_{pq}(y_p-y_q)^2$，这个loss是为了约束相似的超像素块具有相近的深度值。这两者合起来当成一个CRF loss。这个loss形式简单，所以是有close form solution的，可以直接求导。<br>作者用这种超像素分割的方法的弊端在于最后的预测结果也是像超像素一样分成一块一块的<br><img src="https://cl.ly/0w3M1r431O27/Deep%20Convolutional%20Neural%20Fields%20for%20Depth%20Estimation%20from%20a%20Single%20Image_2.png" alt="Deep Convolutional Neural Fields for Depth Estimation from a Single Image_2"><br>还有一点就是用CRF可能会过度平滑预测结果<br><img src="https://cl.ly/3k2Q3l233T1h/Deep%20Convolutional%20Neural%20Fields%20for%20Depth%20Estimation%20from%20a%20Single%20Image_3.png" alt="Deep Convolutional Neural Fields for Depth Estimation from a Single Image_3"><p></p>
<p></p><h2>3. Estimating Depth from Monocular Images as Classification Using Deep Fully Convolutional Residual Networks_arxiv16</h2><br>这篇文章的思想就更简单了，直接借鉴从物体分割的思想【Semantic image segmentation with deep convolutional nets and fully connected crfs】，quantize深度图像，转化成分类问题，然后用crf做后处理<br>虽然作者在loss函数里加了infogain matrix和bootstrapping的参数，然后从实验结果上看几乎没有影响。<br>用CRF优化可能会过度平滑的问题在这篇文章的结果里也体现出来了<br><img src="https://cl.ly/2m433o3R4246/Estimating%20Depth%20from%20Monocular%20Images%20as%20Classification%20Using%20Deep%20Fully%20Convolutional%20Residual%20Networks_1.png" alt="Estimating Depth from Monocular Images as Classification Using Deep Fully Convolutional Residual Networks_1"><br><img src="https://cl.ly/3D3L3d2z0l3o/Estimating%20Depth%20from%20Monocular%20Images%20as%20Classification%20Using%20Deep%20Fully%20Convolutional%20Residual%20Networks_2.png" alt="Estimating Depth from Monocular Images as Classification Using Deep Fully Convolutional Residual Networks_2"><br>目前还没有看到像dialted convolution之类的工作引入进来，另外从这篇文章以及个人经验来看，分类貌似总是比回归要好，用mse来做回归貌似经常不妥当。<p></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章主要是一些做深度图像预测的论文笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="PaperNotes" scheme="http://yoursite.com/categories/PaperNotes/"/>
    
    
      <category term="DepthMap" scheme="http://yoursite.com/tags/DepthMap/"/>
    
  </entry>
  
  <entry>
    <title>Gaussian distribution</title>
    <link href="http://yoursite.com/2016/09/15/Gaussian-distribution/"/>
    <id>http://yoursite.com/2016/09/15/Gaussian-distribution/</id>
    <published>2016-09-15T14:08:26.000Z</published>
    <updated>2016-09-15T14:08:58.148Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要是讲一些常用的高斯分布的性质。那么，为什么高斯分布这么常用呢？我觉得主要有两个原因，一个是中心极限定理；第二个则是因为用高斯分布往往能得到漂亮的闭式解。</p>
<a id="more"></a>
<p>首先，单元和多元的高斯分布分别如下：<br>\begin{equation}<br>\mathcal{N}(x|\mu, \sigma^2)=\frac{1}{(2\pi\sigma^2)^{1/2}}exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}<br>\end{equation}<br>\begin{equation}<br>\mathcal{N}(\mathbf{x}|\mathbf{\mu}, \Sigma)=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\left\{-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})\right\}<br>\end{equation}</p>
<p><strong>1. 在support为$[-\infty,\infty]$以及给定均值和方差的情况下，高斯分布是熵最大的分布</strong><br>首先，熵的定义为<br>\begin{equation}<br>H(x) = -\int p(x)lnp(x)dx<br>\end{equation}<br>给定方差的情况下，$p(x)$需要满足以下几个约束<br>\begin{equation}<br>\int_{-\infty}^{\infty}p(x)dx = 1<br>\end{equation}<br>\begin{equation}<br>\int_{-\infty}^{\infty}xp(x)dx = \mu<br>\end{equation}<br>\begin{equation}<br>\int_{-\infty}^{\infty}(x-\mu)^2p(x)dx = \sigma^2<br>\end{equation}<br>用拉格朗日乘子法，我们需要最大化下面的这个函数<br>\begin{equation}<br>-\int_{-\infty}^{\infty}p(x)lnp(x)dx+\lambda_1\left(\int_{-\infty}^{\infty}p(x)dx-1\right)+\\<br>\lambda_2\left(\int_{-\infty}^{\infty}xp(x)dx - \mu\right)+\lambda_3\left(\int_{-\infty}^{\infty}(x-\mu)^2p(x)dx- \sigma^2\right)<br>\end{equation}<br>用变分法求解【对函数p(x)求导】，可得<br>\begin{equation}<br>p(x)=exp\{-1+\lambda_1+\lambda_2x+\lambda_3(x-\mu)^2\}<br>\end{equation}<br>为了让$\int_Rp(x)dx$有限，所以$\lambda_2$应该等于0，$\lambda_3$应该小于0。这样子，剩下的$\lambda_1$和$\lambda_3$可以通过观察得到<br>\begin{align}<br>\lambda_1 &amp;= 1 - \frac{1}{2}ln(2\pi\sigma^2)\\<br>\lambda_2 &amp;= 0\\<br>\lambda_3 &amp;= -\frac{1}{2\sigma^2}<br>\end{align}<br>所以<br>\begin{equation}<br>p(x)=\frac{1}{(2\pi\sigma^2)^{1/2}}exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}<br>\end{equation}<br>多元的推导需要更多的trick，这里暂时不写</p>
<p><strong>2. 变量的联合分布是高斯，则变量的条件分布和边缘分布也是高斯</strong><br>将变量、均值、协方差矩阵和精度矩阵【precision matrix，协方差矩阵的逆】分块：<br>\begin{equation}<br>x=\begin{bmatrix}<br>    x_a \\<br>    x_b<br>\end{bmatrix},\quad<br>\mu=\begin{bmatrix}<br>    \mu_a \\<br>    \mu_b<br>\end{bmatrix},\quad<br>\Sigma=\begin{bmatrix}<br>    \Sigma_{aa} &amp; \Sigma_{ab}\\<br>    \Sigma_{ba} &amp; \Sigma_{bb}<br>\end{bmatrix},\quad<br>\Lambda=\begin{bmatrix}<br>    \Lambda_{aa} &amp; \Lambda_{ab}\\<br>    \Lambda_{ba} &amp; \Lambda_{bb}<br>\end{bmatrix}<br>\end{equation}<br>这里用精度矩阵只是为了表示上的方便。注意精度矩阵也是对称矩阵【对称矩阵的逆也是对称矩阵】。<br>为了得到$p(x_a|x_b)$和$p(x_a)$，通常采用的方法是配方法【complete the square】。因为高斯分布的的指数可以写成<br>\begin{equation}<br>-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)=\frac{1}{2}x^T\Sigma^{-1}x+x^T\Sigma^{-1}\mu+const<br>\end{equation}<br>所以对于一般的二次型，如果可以写成上面这样的形式，那么我们就可以直接得到我们想要的均值和协方差。<br>我们只讨论$p(x_a|x_b)$，对于$p(x_b|x_a)$的情况，其实是对称的。考虑联合分布$p(x_a,x_b)$指数项里的二次型：<br>\begin{align}<br>\begin{split}<br>&amp;-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) = \\<br>&amp;\quad-\frac{1}{2}(x_a-\mu_a)^T\Lambda_{aa}(x_a-\mu_a) -\frac{1}{2}(x_a-\mu_a)^T\Lambda_{ab}(x_b-\mu_b) \\<br>&amp;\quad-\frac{1}{2}(x_b-\mu_b)^T\Lambda_{ba}(x_a-\mu_a) -\frac{1}{2}(x_b-\mu_b)^T\Lambda_{bb}(x_b-\mu_b)<br>\end{split}<br>\end{align}<br>提取出里面$x_a$的二次项：$-\frac{1}{2}x_a^T\Lambda_{aa}x_a$，所以我们可以直接推断出<br>\begin{equation}<br>\Sigma_{a|b}=\Lambda_{aa}^{-1}<br>\end{equation}<br>现在考虑$x_a$的一次项：<br>\begin{equation}<br>x_a^T\{\Lambda_{aa}\mu_a-\Lambda_{ab}(x_b-\mu_b)\} = x_a^T\Sigma_{a|b}^{-1}\mu_{a|b}<br>\end{equation}<br>所以<br>\begin{align}<br>\mu_{a|b} &amp;= \Sigma_{a|b}\{\Lambda_{aa}\mu_a-\Lambda_{ab}(x_b-\mu_b)\} \\<br>&amp;= \mu_a - \Lambda_{aa}^{-1}\Lambda_{ab}(x_b-\mu_b)<br>\end{align}<br>这里是用分块的精度矩阵来表示的，为了用原来的协方差矩阵来表示，我们需要用到分块矩阵的逆的表达式，即：<br>\begin{equation}<br>\begin{bmatrix}<br>    A &amp; B \\<br>    C &amp; D<br>\end{bmatrix}^{-1}=<br>\begin{bmatrix}<br>    M &amp; -MBD^{-1} \\<br>    -D^{-1}CM &amp; D^{-1}+D^{-1}CMBD^{-1}<br>\end{bmatrix}<br>\end{equation}<br>这里$M=(A-BD^{-1}C)^{-1})$，$M^{-1}$被叫做舒尔补【Schur Complement】。<br>所以<br>\begin{align}<br>\Lambda_{aa} &amp;= (\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1} \\<br>\Lambda_{ab} &amp;= -(\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\Sigma_{ab}\Sigma_{bb}^{-1}<br>\end{align}<br>把上面两式代替到$\mu_{a|b}$和$\Sigma_{a|b}$的表达式中，即可得到用分块的协方差矩阵的表达形式：<br>\begin{align}<br>\mu_{a|b} &amp;= \mu_a + \Sigma_{ab}\Sigma_{bb}^{-1}(x_b-\mu_b)\\<br>\Sigma_{a|b} &amp;= \Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}<br>\end{align}<br>从信息熵的角度来讲，给定$b$的条件下，$a$的不确定性减少了，减少的量与$a,b$之间的协方差的平方成正比——也就是说，$a$和$b$越相关，给定$b$的条件下，$a$的方差越小。<br>注意到$\mu_{a|b}$是$x_b$的线性函数，而$\Sigma_{a|b}$与$x_a$无关。这实际上是线性高斯模型的例子。</p>
<p>接下来考虑边缘分布:$p(x_b)=\int p(x_a,x_b)dx_a$。要把$x_a$积分积掉，那么我们还是先把跟$x_a$相关的项先挑出来，配方，这在上面其实已经做过了，这回把它完整得写出来<br>\begin{equation}<br>-\frac{1}{2}x_a^T\Lambda_{aa}x_a+x_a^T\Lambda_{aa}\mu_{a|b}=-\frac{1}{2}(x_a-\mu_{a|b})^T\Lambda_{aa}(x_a-\mu_{a|b})+\frac{1}{2}\mu_{a|b}^T\Lambda_{aa}\mu_{a|b}<br>\end{equation}<br>注意到$\int exp\{-\frac{1}{2}(x_a-\mu_{a|b})^T\Lambda_{aa}(x_a-\mu_{a|b})\}dx_a$积分得到的其实就是$p(x_a|x_b)$的归一化项的倒数，这是一个常数。这样我们就能把$x_a$积分积掉了。上式中剩下的<br>\begin{align}<br>&amp;\quad \frac{1}{2}\mu_{a|b}^T\Lambda_{aa}\mu_{a|b} \\<br>&amp;= \frac{1}{2}(\mu_a - \Lambda_{aa}^{-1}\Lambda_{ab}(x_b-\mu_b))^T\Lambda_{aa}(\mu_a - \Lambda_{aa}^{-1}\Lambda_{ab}(x_b-\mu_b)) \\<br>&amp;=\frac{1}{2}x_b^T\Lambda_{ba}\Lambda_{aa}^{-1}\Lambda_{ab} -x_b^T\Lambda_{ab}\mu_a - x_b^T\Lambda_{ba}\Lambda_{aa}^{-1}\Lambda_{ab}\mu_a + const<br>\end{align}<br>所以式子(15)中剩下的所有项：<br>\begin{align}<br>&amp;\quad -\frac{1}{2}x_b^T\Lambda_{bb}x_b + x_b^T(\Lambda_{bb}\mu_b+\Lambda_{ba}\mu_a)+const \\<br>&amp;= - \frac{1}{2}x_b^T(\Lambda_{bb} - \Lambda_{ba}\Lambda_{aa}^{-1}\Lambda_{ab})x_b + x_b^T(\Lambda_{bb}- \Lambda_{ba}\Lambda_{aa}^{-1}\Lambda_{ab})u_b + const<br>\end{align}<br>根据式子(14)和(21)，我们很容易得到$p(x_b)$的边缘分布的均值和方差分别就是$\mu_b$和$\Sigma_{bb}$</p>
<p><strong>3. 已知$p(x)$和$p(y|x)$，求$p(y)$和$p(x|y)$</strong><br>假设<br>\begin{align}<br>p(x) &amp;= \mathcal{N}(x|\mu, \Lambda^{-1}) \\<br>p(y|x) &amp;= \mathcal{N}(y|Ax+b,L^{-1})<br>\end{align}<br>注意这是上面所说的线性高斯模型，即$y$的均值与$x$线性相关，而$y$的方差与$x$无关。<br>考虑$x,y$的log联合分布，首先定义$z=[x,y]^T$，则<br>\begin{align}<br>lnp(z) &amp;= lnp(x)+lnp(y|x) \\<br>&amp;= -\frac{1}{2}(x-\mu)^T\Lambda(x-\mu) + \\<br>&amp;\quad -\frac{1}{2}(y-Ax-b)^TL(y-Ax-b)+const<br>\end{align}<br>和第2节一样，注意到这其实是$z$的二次函数，我们可以对上式进行配方，得到$p(z)$的表达式。首先考虑二次项<br>\begin{equation}<br>-\frac{1}{2}(x^T(\Lambda+A^TLA)x + y^TLy - y^TLAx - x^TA^TLy \\<br>= \frac{1}{2}[x, y]<br>\begin{bmatrix}<br>\Lambda+A^TLA &amp; -A^TL \\<br>-LA &amp; L<br>\end{bmatrix}<br>[x,y]^T = \frac{1}{2}z^TR^z<br>\end{equation}<br>也即$z$的高斯分布的精度矩阵为<br>\begin{equation}<br>R=\begin{bmatrix}<br>\Lambda+A^TLA &amp; -A^TL \\<br>-LA &amp; L<br>\end{bmatrix}<br>\end{equation}<br>取逆【按公式(20)直接算】可以得到协方差矩阵<br>\begin{equation}<br>cov[z]=\begin{bmatrix}<br>\Lambda^{-1} &amp; \Lambda^{-1}A^T \\<br>A\Lambda^{-1} &amp; L^{-1}+A\Lambda^{-1}A^T<br>\end{bmatrix}<br>\end{equation}<br>接下来取出一次项<br>\begin{equation}<br>x^T\Lambda\mu-x^TA^TLb+y^TLb=[x, y][\Lambda\mu-A^TLb, Lb]^T<br>\end{equation}<br>根据(14)<br>\begin{equation}<br>E[z]=[\mu, A\mu+b]^T<br>\end{equation}<br>注意到这分别就是对应的$x$和$y$的均值组成的向量</p>
<p>利用从第2节推导的边缘分布的结果，我们可以轻松从联合变量$z$的分布中读出<br>\begin{align}<br>E[y] &amp;= A\mu+b \\<br>cov[y] &amp;= L^{-1}+A\Lambda^{-1}A^T<br>\end{align}<br>注意到这里如果$A=I$，那么$cov[y]$的结果其实就是$cov[y|x]$加上$cov[x]$<br>根据式子(16)、(19)，我们也可以轻松得得到$p(x|y)$的结果<br>\begin{align}<br>E[x|y] &amp;= E[x] - R_{xx}^{-1}R_{xy}(y-E[y]) \\<br>&amp;= \mu - (\Lambda+A^TLA)^{-1}(-A^TL)(y-(A\mu+b)) \\<br>&amp;= (\Lambda+A^TLA)^{-1}(A^TL(y-b)+\Lambda\mu) \\<br>cov[x|y] &amp;= R_{xx}^{-1} = (\Lambda+A^TLA)^{-1}<br>\end{align}<br>这些推导结果在贝叶斯的推理过程中会很有用。</p>
<p><strong>4. 先验【挖坑】</strong><br>(1) 已知方差，均值未知的情况下，高斯分布的先验分布仍然是高斯分布<br>(2) 已知均值，方差未知的情况下，单变量高斯分布的先验分布是inverse gamma分布；多变量高斯分布的先验分布是inverse Wishart分布<br>(3) 均值和方差均未知的情况下，单变量高斯分布的先验分布是inverse normal/gaussian-gamma分布；多变量高斯分布的先验分布是inverse  normal/gaussian-Wishart分布</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章主要是讲一些常用的高斯分布的性质。那么，为什么高斯分布这么常用呢？我觉得主要有两个原因，一个是中心极限定理；第二个则是因为用高斯分布往往能得到漂亮的闭式解。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorial" scheme="http://yoursite.com/categories/Tutorial/"/>
    
    
      <category term="Tutorial" scheme="http://yoursite.com/tags/Tutorial/"/>
    
      <category term="Gaussian" scheme="http://yoursite.com/tags/Gaussian/"/>
    
  </entry>
  
  <entry>
    <title>Distilling the Knowledge in a Neural Network</title>
    <link href="http://yoursite.com/2016/09/11/Distilling-the-Knowledge-in-a-Neural-Network/"/>
    <id>http://yoursite.com/2016/09/11/Distilling-the-Knowledge-in-a-Neural-Network/</id>
    <published>2016-09-11T12:08:49.000Z</published>
    <updated>2016-09-11T12:53:33.015Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章讲的是怎么提取训练好的模型的知识。考虑这样的场景，在很多应用场景中为了提升性能往往需要做集成，但是用集成的模型的话首先部署不够灵活，其次计算量会比较大；或者在深度学习里我们一个模型参数动则上百兆，要把这些模型部署到一些嵌入式设备也不太现实。这篇文章就是对复杂模型的输出【soft target】做一些调整，作为监督信息训练小模型。作者称之为”Knowledge Distillation”。</p>
<a id="more"></a>
<p>分类上我们最常用的目标函数就是softmax，在预测的时候一般就是取预测值最高的那个类别，然而在其他类别上的输出其实蕴含了更多的信息——它反映了当前样本在类别上的相似性。所以我们可以用复杂模型的概率输出——“soft targets”作为监督信息。<strong>如果soft targets的分布蕴含较多的信息【熵比较高】</strong>，那么soft targets能提供比hard targets更多的信息，梯度的variance也会减少，这样我们训练小模型的时候就可以用更少的样本，也能用更高的学习率。如果复杂模型是多个简单模型的集成，那么我们可以对多个简单模型的概率输出做算术或者几何平均，再作为soft targets。</p>
<p>然而在一些比较简单的任务上，如手写字体识别，我们现有的模型往往能对正确类别给予非常高的置信度，导致在其他类别上的概率输出非常得小，这些非常小的概率会对cross entropy loss有非常小的贡献。为解决这样的问题，在[1]中，作者用的是softmax的输入，即logits作为soft targets，然后最小化复杂模型与小模型的logits之间的均方误差。在这篇论文中，作者是在softmax函数里加入一个称为”temperature”的超参$T$，即<br>$$q_i = \frac{exp(z_i/T)}{\sum_jexp(z_j/T)}$$<br>调节$T$使得复杂模型产生一个合适的soft targets，然后用同样的$T$训练小模型；在测试的时候，$T$还是设为1。如果部分训练样本的标签已知，那么我们可以用多目标的联合训练：分别跟soft targets和正确标签算cross entropy loss。因为soft targets的梯度会除以$1/T^2$，所以联合训练的时候需要对soft targets的cross entropy loss乘上$1/T^2$。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章讲的是怎么提取训练好的模型的知识。考虑这样的场景，在很多应用场景中为了提升性能往往需要做集成，但是用集成的模型的话首先部署不够灵活，其次计算量会比较大；或者在深度学习里我们一个模型参数动则上百兆，要把这些模型部署到一些嵌入式设备也不太现实。这篇文章就是对复杂模型的输出【soft target】做一些调整，作为监督信息训练小模型。作者称之为”Knowledge Distillation”。&lt;/p&gt;
    
    </summary>
    
      <category term="ModelCompression" scheme="http://yoursite.com/categories/ModelCompression/"/>
    
    
      <category term="Transfer" scheme="http://yoursite.com/tags/Transfer/"/>
    
      <category term="ModelCompression" scheme="http://yoursite.com/tags/ModelCompression/"/>
    
  </entry>
  
  <entry>
    <title>Variational Inference</title>
    <link href="http://yoursite.com/2016/09/10/Variational-Inference/"/>
    <id>http://yoursite.com/2016/09/10/Variational-Inference/</id>
    <published>2016-09-10T14:42:01.000Z</published>
    <updated>2016-11-17T04:56:41.363Z</updated>
    
    <content type="html"><![CDATA[<h2>1. 背景</h2><br>在概率模型中，我们常常需要得到隐变量的后验分布或者计算相对于某个分布的期望，比如在EM算法中我们需要得到隐变量$Z$的后验分布，以及计算完全数据的似然分布相对于隐变量的后验分布的期望。然而对于很多现实中的模型，常常因为隐变量的维度过高，难以计算；或者期望太过复杂，没有闭式解。这时候我们就要寻求近似解。近似解大体上分为两种，一种是stochastic approximation，如MCMC；另一种是deterministic approximation，比如我们这篇文章要讲的变分推断。<br><br>变分法最早来源于微积分，因为涉及到函数空间，所以叫变分。变分法的核心思想，就是从某个函数空间中找到满足某些条件或约束的函数。我们在统计推断中用到的变分法，实际上就是用形式简单的分布，去近似形式复杂、不易计算的分布，这样再做积分运算就会容易的多。<br><br><a id="more"></a><br><br><br>———-<br><br><h2>2. 基础推导</h2><br>跟EM里的推导一样，似然函数可以推导成一个下界加上一个相对熵的形式：<br>\begin{align}<br>ln(p(X)) &amp;= ln\left(\frac{p(X,Z)}{q(Z)}\right) - ln\left(\frac{p(Z|X)}{q(Z)}\right) \\<br>&amp;=\int q(Z)ln\left(\frac{p(X,Z)}{q(Z)}\right)dZ - \int q(Z)ln\left(\frac{p(Z|X)}{q(Z)}\right)dZ \\<br>&amp;=\underbrace{\int q(Z)ln(p(X,Z))dZ - \int q(Z)ln(q(Z))dZ}_{\mathcal{L}(q)} + \underbrace{\left(-\int q(Z)ln\left(\frac{p(Z|X)}{q(Z)}\right)dZ\right)}_{KL(q||p)} \\<br>&amp;= \mathcal{L}(q) + KL(q||p)<br>\end{align}<br>这里的形式跟EM不同的是参数$\Theta$也包含到了随机变量$Z$里。$\mathcal{L}(q)$叫做Evidence Lower Bound(ELOB)，因为相对熵是恒不小于0的。$\mathcal{L}$是关于函数$q(Z)$的泛函【functional】<br>不同于EM，这里$p(Z|X)$是intractable的，所以在最小化KL divergence的时候，我们需要限制$q(Z)$可选的分布类型——既要tractable，也能提供一个好的approximation。而且，这里不会有”over-fitting”，因为越逼近真实的后验分布越好<br><br>———-<br><br><h2>3. Mean field</h2><br>常用的是限制$q(Z)$为可分解的分布【factorized distributions】，将$Z$分解为$M$组变量$Z_i$，即<br>\begin{equation}<br>q(Z) = \prod_{i=1}^Mq_i(Z_i)<br>\end{equation}<br>这个分解通常与模型相关。在物理上，这种形式的变分推断被称为mean filed theory。<br>将上式的分解代入到$\mathcal{L}(q)$，为了让表达更加简洁明了，用$q_i$表示$q_i(Z_i)$：<br>\begin{align}<br>\mathcal{L}(q) &amp;= \int q(Z)ln(p(X,Z))dZ - \int q(Z)ln(q(Z))dZ \\<br>&amp;= \underbrace{\int \prod_{i=1}^Mq_iln(p(X,Z))dZ}_{part (1)} - \underbrace{\int \prod_{i=1}^Mq_i\sum_{i=1}^Mlnq_idZ}_{part (2)}<br>\end{align}<br><br>\begin{align}<br>(Part\ 1) &amp;= \int \prod_{i=1}^Mq_iln(p(X,Z))dZ \\<br>&amp;= \int_{Z_1}\ldots\int_{Z_M}\prod_{i=1}^Mq_iln(p(X,Z))dZ_1,…,dZ_M \\<br>&amp;= \int_{Z_j}q_j\left(\int_{Z_{i\neq j}}\prod_{i\neq j}^Mq_iln(p(X,Z))\prod_{i\neq j}^MdZ_i\right)dZ_j \\<br>&amp;= \int_{Z_j}q_j\left(\int_{Z_{i\neq j}}ln(p(X,Z))\prod_{i\neq j}^Mq_idZ_i\right)dZ_j \\<br>&amp;= \int_{Z_j}q_j(Z_j)[E_{i\neq j}[ln(p(X,Z))]]dZ_j<br>\end{align}<br><br>\begin{align}<br>(Part\ 2) &amp;= \int \prod_{i=1}^Mq_i\sum_{i=1}^Mln(q_i)dZ \\<br>&amp;= \sum_{j=1}^M\left(\int_{Z_j}q_jln(q_j)\left(\int_{Z_{i\neq j}}\prod_{Z_{i\neq j}}q_idZ_{i\neq j}\right)dZ_j\right) \\<br>&amp;= \sum_{j=1}^M\int_{Z_j}q_jln(q_j)dZ_j<br>\end{align}<br>所以，对于某个特定的$q_j$：<br>\begin{align}<br>\mathcal{L}(q) &amp;= \int_{Z_j}q_j\underbrace{[E_{i\neq j}[ln(p(X,Z))]]}_{ln(\tilde{p}_j(X,Z_j))}dZ_j - \int_{Z_j}q_jln(q_j)dZ_j + \underbrace{const}_{terms\ not\ involve\ q_j} \\<br>&amp;= \int_{Z_j}q_jln\frac{ln(\tilde{p}_j(X,Z_j))}{q_j} + const<br>\end{align}<br>这也是一个负的KL divergence，所以我们可以通过最小化这个KL divergence来最大化$\mathcal{L}(q)$，这时最优的$q^*_j$满足<br>\begin{equation}<br>ln(q_j^*) = E_{i\neq j}[ln(p(X,Z))]<br>\end{equation}<br>这条式子的意思是：因子$q_j$最优解的log为完全数据【观测变量和隐变量】的log联合分布相对于其他因子$q_i,  i\neq j$的期望——这是变分推断的基础。通常我们不需要考虑const那一项，因为const项就是归一化项，归一化项通常可以通过观察得到。<br>我们用坐标下降的方法迭代更新每个因子直到收敛。因为下界对于每个因子都是凸的，所以这个过程保证收敛。<br><br><br>———-<br><h2>4. $KL(p||q)\ vs \ KL(q||p)$</h2><br>上面用的优化是$KL(q||p)$，现在考虑一般情况下用可分解的$q(Z)$最小化$KL(p||q)$的问题：<br>\begin{equation}<br>KL(p||q)=-\int p(Z)\left[\sum_{i=1}^Mln(q_i)\right]dZ+\underbrace{\int p(Z)lnp(Z)dZ}_{const}<br>\end{equation}<br>像上面一样只考虑对某个因子$q_j$做优化，则<br>\begin{align}<br>KL(p||q) &amp;= -\int p(Z)\left[\sum_{i=1}^Mln(q_i)\right]dZ+const \\<br>&amp;= -\int\left(p(Z)ln(q_j)+p(Z)\sum_{i\neq j}ln(q_i)\right)dZ + const \\<br>&amp;= -\int p(Z)ln(q_j)dZ + const \\<br>&amp;= -\int ln(q_j)\underbrace{\left[\int p(Z)\prod_{i\neq j}dZ_i\right]}_{F_j(Z_j)}dZ_j + const \\<br>&amp;= -\int F_j(Z_j)ln(q_j)dZ_j + const<br>\end{align}<br>用拉格朗日乘子法约束$q_j$为一个分布：<br>\begin{equation}<br>-\int F_j(Z_j)ln(q_j)dZ_j + \lambda\left(\int q_jdZ_j -1\right)<br>\end{equation}<br>用变分法的欧拉-拉格朗日方程求解可以得到<br>\begin{equation}<br>-\frac{F_j(Z_j)}{q_j}+\lambda = 0<br>\end{equation}<br>也即<br>\begin{equation}<br>\lambda q_j = F_j(Z_j)<br>\end{equation}<br>两边对$Z_j$积分，可得<br>\begin{equation}<br>\lambda=\int F_j(Z_j)dZ_j=1<br>\end{equation}<br>所以<br>\begin{equation}<br>q_j = F_j(Z_j) = \int p(Z)\prod_{i\neq j}dZ_i = p(Z_j)<br>\end{equation}<br>就是说，在优化$KL(p||q)$的情况下，因子$q_j$的最优解有刚好就是相应的边缘分布$p(Z_j)$<br>PRML里的一幅图描述了对二元高斯分布分别用$KL(q||p)$和$KL(p||q)$优化的结果<br><img src="/uploads/Variational_Inference.png" alt="Variational Inference"><br>左边是$KL(q||p)$，右边是$KL(p||q)$，两种情况下都能很好得拟合均值，然而对于整体分布的拟合情况确有很大差别。这可以从KL divergence的式子里直接考虑<br>\begin{equation}<br>KL(q||p)=-\int q(Z)ln\frac{p(Z)}{q(Z)}dZ<br>\end{equation}<br>这里对值影响比较大的部分主要来自$ln$相除的那部分。对于$KL(q||p)$，在$p(Z)$比较小的地方$q(Z)$也得比较小，不然的话一除，再ln，这个值就会非常大，这叫”zero forcing”；相反，对于$KL(p||q)$的情况，在$p(Z)$比较大的地方，$q(Z)$也得比较大，这种情况叫”zero avoiding”。这就造成了上图左边只在高密度区域有值，而右边则是整体上都有值的结果。<br><br><br>———-<br><h2>5. 极大似然、EM与变分</h2>


<hr>
<h2>6. 指数分布族与变分</h2>


<hr>
<h2>7. Expectation Propagation</h2>


<hr>
]]></content>
    
    <summary type="html">
    
      &lt;h2&gt;1. 背景&lt;/h2&gt;&lt;br&gt;在概率模型中，我们常常需要得到隐变量的后验分布或者计算相对于某个分布的期望，比如在EM算法中我们需要得到隐变量$Z$的后验分布，以及计算完全数据的似然分布相对于隐变量的后验分布的期望。然而对于很多现实中的模型，常常因为隐变量的维度过高，难以计算；或者期望太过复杂，没有闭式解。这时候我们就要寻求近似解。近似解大体上分为两种，一种是stochastic approximation，如MCMC；另一种是deterministic approximation，比如我们这篇文章要讲的变分推断。&lt;br&gt;&lt;br&gt;变分法最早来源于微积分，因为涉及到函数空间，所以叫变分。变分法的核心思想，就是从某个函数空间中找到满足某些条件或约束的函数。我们在统计推断中用到的变分法，实际上就是用形式简单的分布，去近似形式复杂、不易计算的分布，这样再做积分运算就会容易的多。&lt;br&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Tutorial" scheme="http://yoursite.com/categories/Tutorial/"/>
    
    
      <category term="Tutorial" scheme="http://yoursite.com/tags/Tutorial/"/>
    
      <category term="Bayesian" scheme="http://yoursite.com/tags/Bayesian/"/>
    
  </entry>
  
  <entry>
    <title>Self-Paced Learning with Diversity_NIPS14</title>
    <link href="http://yoursite.com/2016/09/04/Self-Paced-Learning-with-Diversity-NIPS14/"/>
    <id>http://yoursite.com/2016/09/04/Self-Paced-Learning-with-Diversity-NIPS14/</id>
    <published>2016-09-04T14:09:31.000Z</published>
    <updated>2016-09-11T12:21:34.454Z</updated>
    
    <content type="html"><![CDATA[<p>这一篇文章的思想其实很简单，就是让SPL在选择样本的时候不单单只考虑样本的难易程度，还要考虑样本的多样性。这个多样性通过group lasso的优化项来体现。</p>
<a id="more"></a>
<p>首先，我们的样本$X=(x_1,…,x_n) \in R^{m_n}$被分成$b$组：$X^{(1)},…,X^{(b)},X^{(j)}\in R^{m_n_j}$，$n_j$是第$j$组的样本数目；这个分组要么是给定的，要么可以用一些无监督的方法，比如聚类得到；相应的定义每个组的难易度系数向量$v=[v^{(1)},…,v^{(b)}],\ v^{(j)}=(v^{(j)}_1,…,v^{(j)}_{n_j})^T\in [0,1]^{n_j}$。这样子得到我们新的优化模型：<br>\begin{equation}<br>min_{w,v}E(w,v;\lambda, \gamma)=\sum_{i=1}^nv_iL(y_i,f(x_i,w))-\lambda\sum_{i=1}^nv_i-\gamma|v|_{2,1},\quad s.t. v\in[0,1]^n<br>\end{equation}<br>这里新引入的负$l_{2,1}-norm$项就是为了得到样本的多样性。具体得<br>$$-|v|_{2,1}=-\sum_{j=1}^b|v^{(j)}|_2$$<br>本来$l_{2,1}-norm$是为了得到组稀疏的，现在加了个负号，就能得到和组稀疏相反的效果，也即多样性。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一篇文章的思想其实很简单，就是让SPL在选择样本的时候不单单只考虑样本的难易程度，还要考虑样本的多样性。这个多样性通过group lasso的优化项来体现。&lt;/p&gt;
    
    </summary>
    
      <category term="SelfPacedLearning" scheme="http://yoursite.com/categories/SelfPacedLearning/"/>
    
    
      <category term="NIPS" scheme="http://yoursite.com/tags/NIPS/"/>
    
      <category term="SelfPacedLearning" scheme="http://yoursite.com/tags/SelfPacedLearning/"/>
    
  </entry>
  
  <entry>
    <title>Expectation Maximization Algorithm</title>
    <link href="http://yoursite.com/2016/09/03/Expectation-Maximization-Algorithm/"/>
    <id>http://yoursite.com/2016/09/03/Expectation-Maximization-Algorithm/</id>
    <published>2016-09-03T02:28:16.000Z</published>
    <updated>2016-09-10T07:57:24.810Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习的领域里面，我们常常需要用极大似然估计【或极大化后验】的方法去做参数估计<br>\begin{equation}<br>\theta^{MLE}=argmax_{\theta}(\mathcal{L}(\theta))=argmax_{\theta}(ln[p(X|\theta)])<br>\end{equation}<br>然而，当模型中含有隐变量，或者说观测数据不完整时，用极大似然估计往往不能得到一个闭式解【closed-form solution】。EM算法就是一种求解这种含有隐变量模型的迭代算法。</p>
<a id="more"></a>
<p>我们用$Z$表示所有的隐变量，$X$表示所有的观察到的变量，$\Theta$表示所有的参数，则log likelihood可以写成：<br>\begin{align}<br>\mathcal{L}(\Theta|X) &amp; = ln(p(X|\Theta))\\<br>&amp;= ln\left(\frac{p(X,Z|\Theta)}{p(Z|X,\Theta)}\right) \\<br>&amp;= ln\left(\frac{p(X,Z|\Theta)}{Q(Z)}*\frac{Q(Z)}{p(Z|X,\Theta)}\right)\\<br>&amp;= ln\left(\frac{p(X,Z|\Theta)}{Q(Z)}\right)+ln\left(\frac{Q(Z)}{p(Z|X,\Theta)}\right)<br>\end{align}<br>两边对Q(Z)这个分布求期望，左边因为不含变量$Z$，所以不会影响：<br>\begin{align}<br>ln(p(X|\Theta)) &amp;= \int_ZQ(Z)ln\left(\frac{p(X,Z|\Theta)}{Q(Z)}\right)+\int_ZQ(Z)ln\left(\frac{Q(Z)}{p(Z|X,\Theta)}\right) \\<br>&amp;= \int_ZQ(Z)ln\left(\frac{p(X,Z|\Theta)}{Q(Z)}\right)+\underbrace{KL(Q(Z)||p(Z|X,\Theta))}_{\ge0} \\<br>&amp;= \mathcal{L}(\Theta,Q)  + KL(Q(Z)||p(Z|X,\Theta)) \\<br>&amp;\ge \mathcal{L}(\Theta,Q)<br>\end{align}<br>也即$L(\Theta,Q)$是$ln(p(X|\Theta))$的下界。引用PRML里的一幅图来形象说明它们之间的关系<br><img src="/uploads/EM.png" alt="EM"><br>所以EM其实是一个Maximize-Maximize的过程【$\Theta^{old}$表示上一次迭代$\Theta$的值】：</p>
<p><ul><b>在E步，我们固定住$\Theta^{old}$，优化下界【with respect to $Q(Z)$】，这时候下界最大当且仅当KL divergence为0，也即$Q(Z)=p(Z|X,\Theta^{old})$；</b></ul></p>
<p><ul><b>在M步，我们固定住$Q(Z)$，优化下界【with respect to $\Theta$】，得到$\Theta^{new}$</b></ul><br>这样子，每一步优化下界都在改进，所以EM一定会收敛，但是EM并不保证收敛到最优解<br>另一种证明$ln(p(X|\Theta))\ge F(\Theta, Z)$的方法是利用琴生不等式【Jensen’s Inequality】：<br>\begin{align}<br>ln(p(X|\Theta) &amp;= ln\int_Zp(X,Z|\Theta) \\<br>&amp;= \underbrace{ln\left(\int_Z\frac{p(X,Z|\Theta)}{Q(Z)}Q(Z)\right)}_{lnE_{Q(Z)}[f(Z)]} \\<br>&amp;\ge \underbrace{\int_Zln\left(\frac{p(X,Z|\Theta)}{Q(Z)}\right)Q(Z)}_{E_{Q(Z)}ln[f(Z)]}<br>\end{align}<br>接下来再讲一下M步里怎么优化下界，做完E步后KL divergence那一项变成0，即$Q(Z)=p(Z|X,\Theta^{old})$，所以<br>\begin{align}<br>\mathcal{L}(\Theta|X) &amp;= \mathcal{L}(\Theta, Q) \\<br>&amp;= \int_ZQ(Z)ln\left(\frac{p(X,Z|\Theta)}{Q(Z)}\right) \\<br>&amp;= \underbrace{\int_Zp(Z,X| \Theta^{old})ln(p(X,Z|\Theta))}_{Q(\Theta, \Theta^{old})}-\underbrace{\int_Zp(Z|X, \Theta^{old})ln(p(Z|X, \Theta^{old}))}_{independent\ of\ \Theta}<br>\end{align}<br>也就是我们只需要最大化$Q(\Theta, \Theta^{old})$<br>注意到现在我们要优化的参数$\Theta$只存在于log里面，如果分布$p(X,Z|\Theta)$是一个指数族分布，那么log和exp将会抵消，我们求解起来就会比原来直接求$p(X|\Theta)$简单得多<br>类似地，EM也可以用来做极大后验估计：<br>\begin{align}<br>ln(p(\Theta|X) &amp;= ln(p(\Theta,X))-ln(p(X)) \\<br>&amp;= ln(p(X|\Theta)) + ln(p(\Theta)) - ln(p(X))<br>\end{align}<br>对$ln(p(X|\Theta))$做和上面一样的分解即可</p>
<p></p><h2>“Tagare” approach</h2><br>另一种证明EM会收敛的方法，大同小异<br>\begin{align}<br>\mathcal{L}(\Theta|X) &amp;= ln(p(X|\Theta)) \\<br>&amp;= ln\left(\frac{p(X,Z|\Theta)}{p(Z|X,\Theta)}\right) \\<br>\end{align}<br>\begin{align}<br>&amp;\Rightarrow \int_zln(p(X|\Theta))p(Z|X, \Theta^{old}) \\<br>&amp;= \int_Zln(p(Z,X|\Theta))p(Z|X, \Theta^{old})dZ-\int_Zln(p(Z|X,\Theta))p(Z|X, \Theta^{old})dZ<br>\end{align}<br>\begin{align}<br>&amp;\Rightarrow ln(p(X|\Theta))\\<br>&amp;= \underbrace{\int_Zp(Z,X| \Theta^{old})ln(p(X,Z|\Theta))}_{Q(\Theta, \Theta^{old})}-\underbrace{\int_Zp(Z|X, \Theta^{old})ln(p(Z|X, \Theta))}_{H(\Theta, \Theta^{old})}<br>\end{align}<br>我们只maximize $Q(\Theta, \Theta^{old})$，因为可以证明<br>\begin{equation}<br>argmax_{\Theta}Q(\Theta, \Theta^{old})=\Theta^{new} \Rightarrow H(\Theta^{new}, \Theta^{old}) \le H(\Theta^{old}, \Theta^{old})<br>\end{equation}<br>这样子，我们就有<br>\begin{equation}<br>\mathcal{L}(\Theta^{new})=Q(\Theta^{new}, \Theta^{old})-H(\Theta^{new}, \Theta^{old}) \ge Q(\Theta^{old}, \Theta^{old})-H(\Theta^{old}, \Theta^{old}) = \mathcal{L}(\Theta^{old})<br>\end{equation}<br>可以用琴生不等式证明$H(\Theta^{old}, \Theta^{old})-H(\Theta, \Theta^{old}) \ge 0\quad \forall\Theta$<p></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在机器学习的领域里面，我们常常需要用极大似然估计【或极大化后验】的方法去做参数估计&lt;br&gt;\begin{equation}&lt;br&gt;\theta^{MLE}=argmax_{\theta}(\mathcal{L}(\theta))=argmax_{\theta}(ln[p(X|\theta)])&lt;br&gt;\end{equation}&lt;br&gt;然而，当模型中含有隐变量，或者说观测数据不完整时，用极大似然估计往往不能得到一个闭式解【closed-form solution】。EM算法就是一种求解这种含有隐变量模型的迭代算法。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorial" scheme="http://yoursite.com/categories/Tutorial/"/>
    
    
      <category term="EM" scheme="http://yoursite.com/tags/EM/"/>
    
      <category term="Tutorial" scheme="http://yoursite.com/tags/Tutorial/"/>
    
      <category term="Latent" scheme="http://yoursite.com/tags/Latent/"/>
    
  </entry>
  
  <entry>
    <title>Self-Paced Learning for Latent Variable Models_NIPS10</title>
    <link href="http://yoursite.com/2016/09/02/Self-Paced-Learning-for-Latent-Variable-Models-NIPS10/"/>
    <id>http://yoursite.com/2016/09/02/Self-Paced-Learning-for-Latent-Variable-Models-NIPS10/</id>
    <published>2016-09-02T14:14:38.000Z</published>
    <updated>2016-09-03T00:51:58.971Z</updated>
    
    <content type="html"><![CDATA[<p>这一篇是Self-Paced Learning(SPL)的奠基之作。<br>SPL，固名思义，就是一步步，有自主步伐节奏得学。Motivation应该来自于09年Bengio提出的Curriculum Learning(CL)。CL受到认知科学的启发——人在学东西的时候也没办法一下子接受特别困难的知识，是从简单的开始学起。所以CL是根据某种先验，将按照困难度排好序的样本逐渐喂给模型。SPL与CL最大的不同之处在于这个排样本的先验是嵌入到模型里面的，是动态的，可以优化学习的。<br>这样子从易到难得学可以看成是一种正则化的手段，有助于加快收敛，并达到一个更好的local minimum.</p>
<a id="more"></a>
<p>考虑一个普通的机器学习优化问题<br>\begin{equation}<br>w_{t+1}=argmin_{w\in R^d}\left(r(w)+\sum_{i=1}^nf(x_i, y_i; w)\right)<br>\end{equation}<br>$r(.)$是一个正则化项，$f(.)$就是loss项了，比如负对数似然函数。SPL需要考虑样本的难易程度，以及一次要用多少样本。所以这篇文章引入了一个binary variable $v_i$，表示这个样本是否是简单的样本，只有简单的样本才在目标函数中有贡献。这样子新的目标函数是一个混合整数规划【mixed-integer program】问题：<br>\begin{equation}<br>(w_{t+1}, v_{t+1})=argmin_{w\in R^d, v\in {0,1}^n}\left(r(w)+\sum_{i=1}^nv_if(x_i, y_i; w) - \frac{1}{K}\sum_{i=1}{n}v_i\right)<br>\end{equation}<br>$K$是用来调整要选多少简单的样本：当$K$比较大时，为了使目标函数更小，那么就只有$f(.)$比较小【high likelihood，因为是负的】的样本会被选中。<br>更重要的是，每个样本是否是简单的其实不是单独考虑的——它们之间通过参数$w$联系起来，也即，一组样本是简单的条件是有$w$能够拟合这组样本，得到一个值比较小的$f(.)$。<br>在训练过程中，我们逐渐减小$K$的值，使得越来越多的样本能够被考虑进来。当$K$趋近于0时，优化(2)其实就相当于在优化(1)。<br>然而(2)这样的混合整数规划问题通常不好求解，所以我们松弛对参数$v$的约束：允许$v_i$的值落在[0, 1]之间。这个松弛是紧的【tight】，也就是说，取得最优值的$w$，对应的每个$v_i$一定是0或1。这很好理解：如果$f(x_i,y_i;w)\lt 1/K$，那么肯定$v_i=1$能取得最小值；如果$f(x_i,y_i;w)\gt 1/K$，那么肯定$v_i=0$能取得最小值。<br>经过这样的松弛之后，问题就比较好解了。特殊情况下，如果$r(.)$和$f(.)$都是凸的，那么这个目标函数就是个biconvex的目标函数，可以采用诸如坐标下降的方法求解。作者用的是alternative convex search(ACS)。一般情况下，如果$r(.)$和$f(.)$都是非凸的，也可以用类似的方法求解：给定$w$，最优的$v$值为$v_i=\delta(f(x_i, y_i; w)\lt 1/K$，其中$\delta(.)$为指示函数【indicator function】；给定$v$，优化(2)就跟优化(1)一样。<br>作者在实验中$w$的初值是设置所有$v_i=1$，然后用CCCP算法跑一段时间。</p>
<p>最后用孟德宇老师的一页ppt总结一下SPL：<br><img src="/uploads/SPL.png" alt="SPL"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一篇是Self-Paced Learning(SPL)的奠基之作。&lt;br&gt;SPL，固名思义，就是一步步，有自主步伐节奏得学。Motivation应该来自于09年Bengio提出的Curriculum Learning(CL)。CL受到认知科学的启发——人在学东西的时候也没办法一下子接受特别困难的知识，是从简单的开始学起。所以CL是根据某种先验，将按照困难度排好序的样本逐渐喂给模型。SPL与CL最大的不同之处在于这个排样本的先验是嵌入到模型里面的，是动态的，可以优化学习的。&lt;br&gt;这样子从易到难得学可以看成是一种正则化的手段，有助于加快收敛，并达到一个更好的local minimum.&lt;/p&gt;
    
    </summary>
    
      <category term="SelfPacedLearning" scheme="http://yoursite.com/categories/SelfPacedLearning/"/>
    
    
      <category term="NIPS" scheme="http://yoursite.com/tags/NIPS/"/>
    
      <category term="Latent" scheme="http://yoursite.com/tags/Latent/"/>
    
      <category term="SelfPacedLearning" scheme="http://yoursite.com/tags/SelfPacedLearning/"/>
    
  </entry>
  
  <entry>
    <title>Domain Adaptation from Multiple Sources via Auxiliary Classifiers_ICML09</title>
    <link href="http://yoursite.com/2016/09/01/Domain-Adaptation-from-Multiple-Sources-via-Auxiliary-Classifiers-ICML09/"/>
    <id>http://yoursite.com/2016/09/01/Domain-Adaptation-from-Multiple-Sources-via-Auxiliary-Classifiers-ICML09/</id>
    <published>2016-09-01T02:51:42.000Z</published>
    <updated>2016-09-01T03:10:24.971Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章做的是从多个源领域到单个目标领域的迁移，思想来源于Adaptive SVM[1]，就是使源领域模型参数“适应”到目标领域去。<br><a id="more"></a><br>假设现在有s个源领域$D^s=(x_i^s, y_i^s)|_{i=1}^{n_s}，s=1,2,…,P$，目标领域有标记数据$D_l^T=(x_i^T,y_i^T)|_{i=1}^{n_l}$和未标记数据$D_u^T=x_i^T|_{i=n_l+1}^{n_l+n_u}$，我们希望源领域的分类器$f^s(x)$和要学的目标领域分类器$f^T(x)$之间满足这样的关系：<br>\begin{equation}<br>f^T(x)=\sum_s\gamma_sf^s(x)+\Delta f(x)\\<br>s.t. \quad \sum_s\gamma_s=1<br>\end{equation}<br>扰动函数【perturbation function】$\Delta f(x)$用目标领域的标记数据$D_l^T$来学，根据[1]，<br>\begin{equation}<br>\Delta f(x)=\sum_{i=1}^{n_l}\alpha_i^Ty_i^Tk(x_i^T,x)<br>\end{equation}<br>类似地，<br>\begin{equation}<br>f^s(x)=\sum_s\gamma_s\sum_{i=1}^{n_s}\gamma_i^sy_i^sk(x_i^s,x)<br>\end{equation}<br>两者用相同的kernel<br>这样子，目标领域的分类器就变成了一些kernel的加权和<br>然而现在我们还没有考虑到怎么利用未标记的数据，根据manifold约束，在流形上相邻的特征它们的decision value也应该相近。因此作者认为，在domain adaption问题中，对于这些未标记的数据，目标分类器的decision value和与它比较相关的源领域分类器的decision value值也不应该差太远，所以作者提出了这样的一个data-dependent regularizer<br>\begin{equation}<br>\Omega_D(\textbf{f}_u^T)=\frac{1}{2}\sum_{i=n_l+1}^{n_T}\sum_s\gamma_s(f_i^T-f_i^s)^2=\frac{1}{2}\sum_s\gamma_s|\textbf{f}_u^T-\textbf{f}_u^s|^2<br>\end{equation}<br>$\gamma_s$表示目标领域与某个源领域之间的相关程度<br>$\textbf{f}_u^T=[f_{n_{l+1}}^T,…,f_{n_T}^T]’,\ \textbf{f}_u^s=[f_{n_{l+1}}^s, …, f_{n_T}^s]’$<br>所以最后的目标函数是<br>\begin{equation}<br>min_{f^T}\Omega(f^T)+\frac{1}{2}\sum_{i=1}^{n_l}(f_i^T-y_i^T)^2+\Omega_D(\textbf{f}_u^T)<br>\end{equation}<br>其中，$\Omega(f^T)$是目标分类器的参数正则化项<br>最后，作者还提出因为SVR【Support Vector Regression】通常能得到比较稀疏的解，所以还可以再加一个约束项<br>作者最后得出来的对偶形式也没有涉及太多的kernel运算，所以作者认为自己的算法比较scale</p>
<p>实验是在TRECVID 2005数据集上进行的</p>
<hr>
<p>[1] Crossdomain video concept detection using adaptive SVMs</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章做的是从多个源领域到单个目标领域的迁移，思想来源于Adaptive SVM[1]，就是使源领域模型参数“适应”到目标领域去。&lt;br&gt;
    
    </summary>
    
      <category term="TransferLearning" scheme="http://yoursite.com/categories/TransferLearning/"/>
    
    
      <category term="Transfer" scheme="http://yoursite.com/tags/Transfer/"/>
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
      <category term="ICML" scheme="http://yoursite.com/tags/ICML/"/>
    
  </entry>
  
  <entry>
    <title>Bilinear classifiers for visual recognition_NIPS09</title>
    <link href="http://yoursite.com/2016/08/31/Bilinear-classifiers-for-visual-recognition-NIPS09/"/>
    <id>http://yoursite.com/2016/08/31/Bilinear-classifiers-for-visual-recognition-NIPS09/</id>
    <published>2016-08-31T11:39:59.000Z</published>
    <updated>2016-09-01T03:03:19.187Z</updated>
    
    <content type="html"><![CDATA[<p>Bilinear模型作为linear模型的泛化版本，一般是为了提升模型的表达能力，但是这篇文章是反过来的：作者把参数$W$分解为一些低秩矩阵，也即“因子”【factor】的积，进而减少参数数量，防止过拟合。<br><a id="more"></a><br>具体得说，原本线性SVM的模型是<br>\begin{equation} \label{eq:1}<br>L(W)=\frac{1}{2}Tr(W^TW)+C\sum_nmax(0,1-y_nTr(W^TX_n)<br>\end{equation}<br>$X \in R^{n_y*n_x*n_f}$, $n_y$和$n_x$是空间维度，$n_f$是特征维度。为简化讨论，下面固定$n_f=1$<br>通过对W进行低秩分解【这里的秩的约束通过$W_x$和$W_y$的秩不会超过$d$来体现】<br>\begin{equation}<br>W=W_yW^T_x,\ where\ \ W_y\in R^{n_y*d}, W_x\in R^{n_x*d}<br>\end{equation}<br>这样子对$W$进行分解后的Bilinear SVM可以重新写成<br>\begin{equation}<br>L_(W_y, W_x)=\frac{1}{2}Tr(W_xW_y^TW_yW_x^T)+C\sum_nmax(0, 1-y_nTr(W_xW_y^TX_n))<br>\end{equation}<br>然而这个形式不好优化，我们得把它写成像公式$\ref{eq:1}$的形式，即<br>\begin{equation}<br>L(\tilde{W}_y, W_x)=\frac{1}{2}Tr(\tilde{W}_y^T\tilde{W}_y)+C\sum_nmax(0,1-y_nTr(\tilde{W}_y^T\tilde{X}_n))\\<br>where\ \tilde{W}_y=W_yA^{\frac{1}{2}}, \tilde{X}_n=X_nW_xA^{-\frac{1}{2}}, A=W_x^TW_x<br>\end{equation}<br>这是一个biconvex的目标函数，可以用coordinate descent来解<br>如果现在有$M$个任务，每个任务都有不同的数据，那么我们限定所有的任务共享相同的参数空间，即<br>\begin{equation}<br>L(W)=\frac{1}{2}\sum_mTr(W^{T}W)+\sum_mC_m\sum_nmax(0, 1-y_n^mTr(W^{T}X_n^m))<br>\end{equation}<br>这个模型还可以扩展成Multilinear的模型<br>其他的SVM模型，如Structural SVM，也可以写成这种形式</p>
<p></p><h2>实验</h2><br>作者使用了行人检测的数据集INRIA-MOTION[1]类的数据集UCF-Sports[2]，使用的是HOG特征和流特征flow feature[1]<p></p>
<p>[1] Human detection using oriented histograms of flow and appearance<br>[2] Action MACH a spatio-temporal Maximum Average Correlation Height filter for action recognition</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bilinear模型作为linear模型的泛化版本，一般是为了提升模型的表达能力，但是这篇文章是反过来的：作者把参数$W$分解为一些低秩矩阵，也即“因子”【factor】的积，进而减少参数数量，防止过拟合。&lt;br&gt;
    
    </summary>
    
      <category term="TransferLearning" scheme="http://yoursite.com/categories/TransferLearning/"/>
    
    
      <category term="Transfer" scheme="http://yoursite.com/tags/Transfer/"/>
    
      <category term="Bilinear" scheme="http://yoursite.com/tags/Bilinear/"/>
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
      <category term="NIPS" scheme="http://yoursite.com/tags/NIPS/"/>
    
  </entry>
  
</feed>
