<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>K_Augus</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-09-11T12:22:27.760Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>K_Augus</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Distilling the Knowledge in a Neural Network</title>
    <link href="http://yoursite.com/2016/09/11/Distilling-the-Knowledge-in-a-Neural-Network/"/>
    <id>http://yoursite.com/2016/09/11/Distilling-the-Knowledge-in-a-Neural-Network/</id>
    <published>2016-09-11T12:08:49.000Z</published>
    <updated>2016-09-11T12:22:27.760Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章讲的是怎么提取训练好的模型的知识。考虑这样的场景，在很多应用场景中为了提升性能往往需要做集成，但是用集成的模型的话首先部署不够灵活，其次计算量会比较大；或者在深度学习里我们一个模型参数动则上百兆，要把这些模型部署到一些嵌入式设备也不太现实。这篇文章就是对复杂模型的输出【soft target】做一些调整，作为监督信息训练小模型。作者称之为”Knowledge Distillation”。</p>
<a id="more"></a>
<p>分类上我们最常用的目标函数就是softmax，在预测的时候一般就是取预测值最高的那个类别，然而在其他类别上的输出其实蕴含了更多的信息——它反映了当前样本在类别上的相似性。所以我们可以用复杂模型的概率输出——“soft targets”作为监督信息。<strong>如果soft targets的分布蕴含较多的信息【熵比较高】</strong>，那么soft targets能提供比hard targets更多的信息，梯度的variance也会减少，这样我们训练小模型的时候就可以用更少的样本，也能用更高的学习率。如果复杂模型是多个简单模型的集成，那么我们可以对多个简单模型的概率输出做算术或者几何平均，再作为soft targets。</p>
<p>然而在一些比较简单的任务上，如手写字体识别，我们现有的模型往往能对正确类别给予非常高的置信度，导致在其他类别上的概率输出非常得小，这些非常小的概率会对cross entropy loss有非常小的贡献。为解决这样的问题，在[1]中，作者用的是softmax的输入，即logits作为soft targets，然后最小化复杂模型与小模型的logits之间的均方误差。在这篇论文中，作者是在softmax函数里加入一个称为”temperature”的超参$T$，即<br>$$q_i = \frac{exp(z_i/T)}{\sum_jexp(z_j/T)}$$<br>调节$T$使得复杂模型产生一个合适的soft targets，然后用同样的$T$训练小模型；在测试的时候，$T$还是设为1。如果部分训练样本的标签已知，那么我们可以用多目标的联合训练：分别跟soft targets和正确标签算cross entropy loss。因为soft targets的梯度会除以$1/T^2$，所以联合训练的时候需要对soft targets的cross entropy loss乘上$1/T^2$。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章讲的是怎么提取训练好的模型的知识。考虑这样的场景，在很多应用场景中为了提升性能往往需要做集成，但是用集成的模型的话首先部署不够灵活，其次计算量会比较大；或者在深度学习里我们一个模型参数动则上百兆，要把这些模型部署到一些嵌入式设备也不太现实。这篇文章就是对复杂模型的输出【soft target】做一些调整，作为监督信息训练小模型。作者称之为”Knowledge Distillation”。&lt;/p&gt;
    
    </summary>
    
      <category term="ModelCompression" scheme="http://yoursite.com/categories/ModelCompression/"/>
    
    
      <category term="Transfer" scheme="http://yoursite.com/tags/Transfer/"/>
    
      <category term="ModelCompression" scheme="http://yoursite.com/tags/ModelCompression/"/>
    
  </entry>
  
  <entry>
    <title>Variational Inference</title>
    <link href="http://yoursite.com/2016/09/10/Variational-Inference/"/>
    <id>http://yoursite.com/2016/09/10/Variational-Inference/</id>
    <published>2016-09-10T14:42:01.000Z</published>
    <updated>2016-09-10T14:46:20.536Z</updated>
    
    <content type="html"><![CDATA[<h2>1. 背景</h2><br>在概率模型中，我们常常需要得到隐变量的后验分布或者计算相对于某个分布的期望，比如在EM算法中我们需要得到隐变量$Z$的后验分布，以及计算完全数据的似然分布相对于隐变量的后验分布的期望。然而对于很多现实中的模型，常常因为隐变量的维度过高，难以计算；或者期望太过复杂，没有闭式解。这时候我们就要寻求近似解。近似解大体上分为两种，一种是stochastic approximation，如MCMC；另一种是deterministic approximation，比如我们这篇文章要讲的变分推断。<br><br>变分法最早来源于微积分，因为涉及到函数空间，所以叫变分。变分法的核心思想，就是从某个函数空间中找到满足某些条件或约束的函数。我们在统计推断中用到的变分法，实际上就是用形式简单的分布，去近似形式复杂、不易计算的分布，这样再做积分运算就会容易的多。<br><br><a id="more"></a><br><br><br>———-<br><br><h2>2. 基础推导</h2><br>跟EM里的推导一样，似然函数可以推导成一个下界加上一个相对熵的形式：<br>\begin{align}<br>ln(p(X)) &amp;= ln\left(\frac{p(X,Z)}{q(Z)}\right) - ln\left(\frac{p(Z|X)}{q(Z)}\right) \\<br>&amp;=\int q(Z)ln\left(\frac{p(X,Z)}{q(Z)}\right)dZ - \int q(Z)ln\left(\frac{p(Z|X)}{q(Z)}\right)dZ \\<br>&amp;=\underbrace{\int q(Z)ln(p(X,Z))dZ - \int q(Z)ln(q(Z))dZ}_{\mathcal{L}(q)} + \underbrace{\left(-int q(Z)ln\left(\frac{p(Z|X)}{q(Z)}\right)dZ\right)}_{KL(q||p)} \\<br>&amp;= \mathcal{L}(q) + KL(q||p)<br>\end{align}<br>这里的形式跟EM不同的是参数$\Theta$也包含到了随机变量$Z$里。$\mathcal{L}(q)$叫做Evidence Lower Bound(ELOB)，因为相对熵是恒不小于0的。$mathcal{L}$是关于函数$q(Z)$的泛函【functional】<br>不同于EM，这里$p(Z|X)$是intractable的，所以在最小化KL divergence的时候，我们需要限制$q(Z)$可选的分布类型——既要tractable，也能提供一个好的approximation。而且，这里不会有”over-fitting”，因为越逼近真实的后验分布越好<br><br>———-<br><br><h2>3. Mean field</h2><br>常用的是限制$q(Z)$为可分解的分布【factorized distributions】，将$Z$分解为$M$组变量$Z_i$，即<br>\begin{equation}<br>q(Z) = \prod_{i=1}^Mq_i(Z_i)<br>\end{equation}<br>这个分解通常与模型相关。在物理上，这种形式的变分推断被称为mean filed theory。<br>将上式的分解代入到$\mathcal{L}(q)$，为了让表达更加简洁明了，用$q_i$表示$q_i(Z_i)$：<br>\begin{align}<br>\mathcal{L}(q) &amp;= \int q(Z)ln(p(X,Z))dZ - \int q(Z)ln(q(Z))dZ \\<br>&amp;= \underbrace{\int \prod_{i=1}^Mq_iln(p(X,Z))dZ}_{part (1)} - \underbrace{\int \prod_{i=1}^Mq_i\sum_{i=1}^Mlnq_idZ}_{part (2)}<br>\end{align}<br><br>\begin{align}<br>(Part\ 1) &amp;= \int \prod_{i=1}^Mq_iln(p(X,Z))dZ \\<br>&amp;= \int_{Z_1}\ldots\int_{Z_M}\prod_{i=1}^Mq_iln(p(X,Z))dZ_1,…,dZ_M \\<br>&amp;= \int_{Z_j}q_j\left(\int_{Z_{i\neq j}}\prod_{i\neq j}^Mq_iln(p(X,Z))\prod_{i\neq j}^MdZ_i\right)dZ_j \\<br>&amp;= \int_{Z_j}q_j\left(\int_{Z_{i\neq j}}ln(p(X,Z))\prod_{i\neq j}^Mq_idZ_i\right)dZ_j \\<br>&amp;= \int_{Z_j}q_j(Z_j)[E_{i\neg j}[ln(p(X,Z))]]dZ_j<br>\end{align}<br><br>\begin{align}<br>(Part\ 2) &amp;= \int \prod_{i=1}^Mq_i\sum_{i=1}^Mln(q_i)dZ \\<br>&amp;= \sum_{j=1}^M\left(\int_{Z_j}q_jln(q_j)\left(\int_{Z_{i\neq j}}\prod_{Z_{i\neg j}}q_idZ_{i\neq j}\right)dZ_j\right) \\<br>&amp;= \sum_{j=1}^M\int_{Z_j}q_jln(q_j)dZ_j<br>\end{align}<br>所以，对于某个特定的$q_j$：<br>\begin{align}<br>\mathcal{L}(q) &amp;= \int_{Z_j}q_j\underbrace{[E_{i\neg j}[ln(p(X,Z))]]}_{ln(\tilde{p}_j(X,Z_j))}dZ_j - \int_{Z_j}q_jln(q_j)dZ_j + \underbrace{const}_{terms\ not\ involve\ q_j} \\<br>&amp;= \int_{Z_j}q_jln\frac{ln(\tilde{p}_j(X,Z_j))}{q_j} + const<br>\end{align}<br>这也是一个负的KL divergence，所以我们可以通过最小化这个KL divergence来最大化$\mathcal{L}(q)$，这时最优的$q^*_j$满足<br>\begin{equation}<br>ln(q_j^*) = E_{i\neg j}[ln(p(X,Z))]<br>\end{equation}<br>这条式子的意思是：因子$q_j$最优解的log为完全数据【观测变量和隐变量】的log联合分布相对于其他因子$q_i,  i\neq j$的期望——这是变分推断的基础。通常我们不需要考虑const那一项，因为const项就是归一化项，归一化项通常可以通过观察得到。<br>我们用坐标下降的方法迭代更新每个因子直到收敛。因为下界对于每个因子都是凸的，所以这个过程保证收敛。<br><br><br>———-<br><h2>4. $KL(p||q)\ vs \ KL(q||p)$</h2><br>上面用的优化是$KL(q||p)$，现在考虑一般情况下用可分解的$q(Z)$最小化$KL(p||q)$的问题：<br>\begin{equation}<br>KL(p||q)=-\int p(Z)\left[\sum_{i=1}^Mln(q_i)\right]dZ+\underbrace{\int p(Z)lnp(Z)dZ}_{const}<br>\end{equation}<br>像上面一样只考虑对某个因子$q_j$做优化，则<br>\begin{align}<br>KL(p||q) &amp;= -\int p(Z)\left[\sum_{i=1}^Mln(q_i)\right]dZ+const \\<br>&amp;= -\int\left(p(Z)ln(q_j)+p(Z)\sum_{i\neg j}ln(q_i)\right)dZ + const \\<br>&amp;= -\int p(Z)ln(q_j)dZ + const \\<br>&amp;= -\int ln(q_j)\underbrace{\left[\int p(Z)\prod_{i\neg j}dZ_i\right]}_{F_j(Z_j)}dZ_j + const \\<br>&amp;= -\int F_j(Z_j)ln(q_j)dZ_j + const<br>\end{align}<br>用拉格朗日乘子法约束$q_j$为一个分布：<br>\begin{equation}<br>-\int F_j(Z_j)ln(q_j)dZ_j + \lambda\left(\int q_jdZ_j -1\right)<br>\end{equation}<br>用变分法的欧拉-拉格朗日方程求解可以得到<br>\begin{equation}<br>-\frac{F_j(Z_j)}{q_j}+\lambda = 0<br>\end{equation}<br>也即<br>\begin{equation}<br>\lambda q_j = F_j(Z_j)<br>\end{equation}<br>两边对$Z_j$积分，可得<br>\begin{equation}<br>\lambda=\int F_j(Z_j)dZ_j=1<br>\end{equation}<br>所以<br>\begin{equation}<br>q_j = F_j(Z_j) = \int p(Z)\prod_{i\neg j}dZ_i = p(Z_j)<br>\end{equation}<br>就是说，在优化$KL(p||q)$的情况下，因子$q_j$的最优解有刚好就是相应的边缘分布$p(Z_j)$<br>PRML里的一幅图描述了对二元高斯分布分别用$KL(q||p)$和$KL(p||q)$优化的结果<br><img src="/uploads/Variational_Inference.png" alt="Variational Inference"><br>左边是$KL(q||p)$，右边是$KL(p||q)$，两种情况下都能很好得拟合均值，然而对于整体分布的拟合情况确有很大差别。这可以从KL divergence的式子里直接考虑<br>\begin{equation}<br>KL(q||p)=-\int q(Z)ln\frac{p(Z)}{q(Z)}dZ<br>\end{equation}<br>这里对值影响比较大的部分主要来自$ln$相除的那部分。对于$KL(q||p)$，在$p(Z)$比较小的地方$q(Z)$也得比较小，不然的话一除，再ln，这个值就会非常大，这叫”zero forcing”；相反，对于$KL(p||q)$的情况，在$p(Z)$比较大的地方，$q(Z)$也得比较大，这种情况叫”zero avoiding”。这就造成了上图左边只在高密度区域有值，而右边则是整体上都有值的结果。<br><br><br>———-<br><h2>5. 极大似然、EM与变分</h2>


<hr>
<h2>6. 指数分布族与变分</h2>


<hr>
<h2>7. Expectation Propagation</h2>


<hr>
]]></content>
    
    <summary type="html">
    
      &lt;h2&gt;1. 背景&lt;/h2&gt;&lt;br&gt;在概率模型中，我们常常需要得到隐变量的后验分布或者计算相对于某个分布的期望，比如在EM算法中我们需要得到隐变量$Z$的后验分布，以及计算完全数据的似然分布相对于隐变量的后验分布的期望。然而对于很多现实中的模型，常常因为隐变量的维度过高，难以计算；或者期望太过复杂，没有闭式解。这时候我们就要寻求近似解。近似解大体上分为两种，一种是stochastic approximation，如MCMC；另一种是deterministic approximation，比如我们这篇文章要讲的变分推断。&lt;br&gt;&lt;br&gt;变分法最早来源于微积分，因为涉及到函数空间，所以叫变分。变分法的核心思想，就是从某个函数空间中找到满足某些条件或约束的函数。我们在统计推断中用到的变分法，实际上就是用形式简单的分布，去近似形式复杂、不易计算的分布，这样再做积分运算就会容易的多。&lt;br&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Tutorial" scheme="http://yoursite.com/categories/Tutorial/"/>
    
    
      <category term="Tutorial" scheme="http://yoursite.com/tags/Tutorial/"/>
    
      <category term="Bayesian" scheme="http://yoursite.com/tags/Bayesian/"/>
    
  </entry>
  
  <entry>
    <title>Self-Paced Learning with Diversity_NIPS14</title>
    <link href="http://yoursite.com/2016/09/04/Self-Paced-Learning-with-Diversity-NIPS14/"/>
    <id>http://yoursite.com/2016/09/04/Self-Paced-Learning-with-Diversity-NIPS14/</id>
    <published>2016-09-04T14:09:31.000Z</published>
    <updated>2016-09-11T12:21:34.454Z</updated>
    
    <content type="html"><![CDATA[<p>这一篇文章的思想其实很简单，就是让SPL在选择样本的时候不单单只考虑样本的难易程度，还要考虑样本的多样性。这个多样性通过group lasso的优化项来体现。</p>
<a id="more"></a>
<p>首先，我们的样本$X=(x_1,…,x_n) \in R^{m_n}$被分成$b$组：$X^{(1)},…,X^{(b)},X^{(j)}\in R^{m_n_j}$，$n_j$是第$j$组的样本数目；这个分组要么是给定的，要么可以用一些无监督的方法，比如聚类得到；相应的定义每个组的难易度系数向量$v=[v^{(1)},…,v^{(b)}],\ v^{(j)}=(v^{(j)}_1,…,v^{(j)}_{n_j})^T\in [0,1]^{n_j}$。这样子得到我们新的优化模型：<br>\begin{equation}<br>min_{w,v}E(w,v;\lambda, \gamma)=\sum_{i=1}^nv_iL(y_i,f(x_i,w))-\lambda\sum_{i=1}^nv_i-\gamma|v|_{2,1},\quad s.t. v\in[0,1]^n<br>\end{equation}<br>这里新引入的负$l_{2,1}-norm$项就是为了得到样本的多样性。具体得<br>$$-|v|_{2,1}=-\sum_{j=1}^b|v^{(j)}|_2$$<br>本来$l_{2,1}-norm$是为了得到组稀疏的，现在加了个负号，就能得到和组稀疏相反的效果，也即多样性。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一篇文章的思想其实很简单，就是让SPL在选择样本的时候不单单只考虑样本的难易程度，还要考虑样本的多样性。这个多样性通过group lasso的优化项来体现。&lt;/p&gt;
    
    </summary>
    
      <category term="SelfPacedLearning" scheme="http://yoursite.com/categories/SelfPacedLearning/"/>
    
    
      <category term="NIPS" scheme="http://yoursite.com/tags/NIPS/"/>
    
      <category term="SelfPacedLearning" scheme="http://yoursite.com/tags/SelfPacedLearning/"/>
    
  </entry>
  
  <entry>
    <title>Expectation Maximization Algorithm</title>
    <link href="http://yoursite.com/2016/09/03/Expectation-Maximization-Algorithm/"/>
    <id>http://yoursite.com/2016/09/03/Expectation-Maximization-Algorithm/</id>
    <published>2016-09-03T02:28:16.000Z</published>
    <updated>2016-09-10T07:57:24.810Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习的领域里面，我们常常需要用极大似然估计【或极大化后验】的方法去做参数估计<br>\begin{equation}<br>\theta^{MLE}=argmax_{\theta}(\mathcal{L}(\theta))=argmax_{\theta}(ln[p(X|\theta)])<br>\end{equation}<br>然而，当模型中含有隐变量，或者说观测数据不完整时，用极大似然估计往往不能得到一个闭式解【closed-form solution】。EM算法就是一种求解这种含有隐变量模型的迭代算法。</p>
<a id="more"></a>
<p>我们用$Z$表示所有的隐变量，$X$表示所有的观察到的变量，$\Theta$表示所有的参数，则log likelihood可以写成：<br>\begin{align}<br>\mathcal{L}(\Theta|X) &amp; = ln(p(X|\Theta))\\<br>&amp;= ln\left(\frac{p(X,Z|\Theta)}{p(Z|X,\Theta)}\right) \\<br>&amp;= ln\left(\frac{p(X,Z|\Theta)}{Q(Z)}*\frac{Q(Z)}{p(Z|X,\Theta)}\right)\\<br>&amp;= ln\left(\frac{p(X,Z|\Theta)}{Q(Z)}\right)+ln\left(\frac{Q(Z)}{p(Z|X,\Theta)}\right)<br>\end{align}<br>两边对Q(Z)这个分布求期望，左边因为不含变量$Z$，所以不会影响：<br>\begin{align}<br>ln(p(X|\Theta)) &amp;= \int_ZQ(Z)ln\left(\frac{p(X,Z|\Theta)}{Q(Z)}\right)+\int_ZQ(Z)ln\left(\frac{Q(Z)}{p(Z|X,\Theta)}\right) \\<br>&amp;= \int_ZQ(Z)ln\left(\frac{p(X,Z|\Theta)}{Q(Z)}\right)+\underbrace{KL(Q(Z)||p(Z|X,\Theta))}_{\ge0} \\<br>&amp;= \mathcal{L}(\Theta,Q)  + KL(Q(Z)||p(Z|X,\Theta)) \\<br>&amp;\ge \mathcal{L}(\Theta,Q)<br>\end{align}<br>也即$L(\Theta,Q)$是$ln(p(X|\Theta))$的下界。引用PRML里的一幅图来形象说明它们之间的关系<br><img src="/uploads/EM.png" alt="EM"><br>所以EM其实是一个Maximize-Maximize的过程【$\Theta^{old}$表示上一次迭代$\Theta$的值】：</p>
<p><ul><b>在E步，我们固定住$\Theta^{old}$，优化下界【with respect to $Q(Z)$】，这时候下界最大当且仅当KL divergence为0，也即$Q(Z)=p(Z|X,\Theta^{old})$；</b></ul></p>
<p><ul><b>在M步，我们固定住$Q(Z)$，优化下界【with respect to $\Theta$】，得到$\Theta^{new}$</b></ul><br>这样子，每一步优化下界都在改进，所以EM一定会收敛，但是EM并不保证收敛到最优解<br>另一种证明$ln(p(X|\Theta))\ge F(\Theta, Z)$的方法是利用琴生不等式【Jensen’s Inequality】：<br>\begin{align}<br>ln(p(X|\Theta) &amp;= ln\int_Zp(X,Z|\Theta) \\<br>&amp;= \underbrace{ln\left(\int_Z\frac{p(X,Z|\Theta)}{Q(Z)}Q(Z)\right)}_{lnE_{Q(Z)}[f(Z)]} \\<br>&amp;\ge \underbrace{\int_Zln\left(\frac{p(X,Z|\Theta)}{Q(Z)}\right)Q(Z)}_{E_{Q(Z)}ln[f(Z)]}<br>\end{align}<br>接下来再讲一下M步里怎么优化下界，做完E步后KL divergence那一项变成0，即$Q(Z)=p(Z|X,\Theta^{old})$，所以<br>\begin{align}<br>\mathcal{L}(\Theta|X) &amp;= \mathcal{L}(\Theta, Q) \\<br>&amp;= \int_ZQ(Z)ln\left(\frac{p(X,Z|\Theta)}{Q(Z)}\right) \\<br>&amp;= \underbrace{\int_Zp(Z,X| \Theta^{old})ln(p(X,Z|\Theta))}_{Q(\Theta, \Theta^{old})}-\underbrace{\int_Zp(Z|X, \Theta^{old})ln(p(Z|X, \Theta^{old}))}_{independent\ of\ \Theta}<br>\end{align}<br>也就是我们只需要最大化$Q(\Theta, \Theta^{old})$<br>注意到现在我们要优化的参数$\Theta$只存在于log里面，如果分布$p(X,Z|\Theta)$是一个指数族分布，那么log和exp将会抵消，我们求解起来就会比原来直接求$p(X|\Theta)$简单得多<br>类似地，EM也可以用来做极大后验估计：<br>\begin{align}<br>ln(p(\Theta|X) &amp;= ln(p(\Theta,X))-ln(p(X)) \\<br>&amp;= ln(p(X|\Theta)) + ln(p(\Theta)) - ln(p(X))<br>\end{align}<br>对$ln(p(X|\Theta))$做和上面一样的分解即可</p>
<p></p><h2>“Tagare” approach</h2><br>另一种证明EM会收敛的方法，大同小异<br>\begin{align}<br>\mathcal{L}(\Theta|X) &amp;= ln(p(X|\Theta)) \\<br>&amp;= ln\left(\frac{p(X,Z|\Theta)}{p(Z|X,\Theta)}\right) \\<br>\end{align}<br>\begin{align}<br>&amp;\Rightarrow \int_zln(p(X|\Theta))p(Z|X, \Theta^{old}) \\<br>&amp;= \int_Zln(p(Z,X|\Theta))p(Z|X, \Theta^{old})dZ-\int_Zln(p(Z|X,\Theta))p(Z|X, \Theta^{old})dZ<br>\end{align}<br>\begin{align}<br>&amp;\Rightarrow ln(p(X|\Theta))\\<br>&amp;= \underbrace{\int_Zp(Z,X| \Theta^{old})ln(p(X,Z|\Theta))}_{Q(\Theta, \Theta^{old})}-\underbrace{\int_Zp(Z|X, \Theta^{old})ln(p(Z|X, \Theta))}_{H(\Theta, \Theta^{old})}<br>\end{align}<br>我们只maximize $Q(\Theta, \Theta^{old})$，因为可以证明<br>\begin{equation}<br>argmax_{\Theta}Q(\Theta, \Theta^{old})=\Theta^{new} \Rightarrow H(\Theta^{new}, \Theta^{old}) \le H(\Theta^{old}, \Theta^{old})<br>\end{equation}<br>这样子，我们就有<br>\begin{equation}<br>\mathcal{L}(\Theta^{new})=Q(\Theta^{new}, \Theta^{old})-H(\Theta^{new}, \Theta^{old}) \ge Q(\Theta^{old}, \Theta^{old})-H(\Theta^{old}, \Theta^{old}) = \mathcal{L}(\Theta^{old})<br>\end{equation}<br>可以用琴生不等式证明$H(\Theta^{old}, \Theta^{old})-H(\Theta, \Theta^{old}) \ge 0\quad \forall\Theta$<p></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在机器学习的领域里面，我们常常需要用极大似然估计【或极大化后验】的方法去做参数估计&lt;br&gt;\begin{equation}&lt;br&gt;\theta^{MLE}=argmax_{\theta}(\mathcal{L}(\theta))=argmax_{\theta}(ln[p(X|\theta)])&lt;br&gt;\end{equation}&lt;br&gt;然而，当模型中含有隐变量，或者说观测数据不完整时，用极大似然估计往往不能得到一个闭式解【closed-form solution】。EM算法就是一种求解这种含有隐变量模型的迭代算法。&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorial" scheme="http://yoursite.com/categories/Tutorial/"/>
    
    
      <category term="EM" scheme="http://yoursite.com/tags/EM/"/>
    
      <category term="Tutorial" scheme="http://yoursite.com/tags/Tutorial/"/>
    
      <category term="Latent" scheme="http://yoursite.com/tags/Latent/"/>
    
  </entry>
  
  <entry>
    <title>Self-Paced Learning for Latent Variable Models_NIPS10</title>
    <link href="http://yoursite.com/2016/09/02/Self-Paced-Learning-for-Latent-Variable-Models-NIPS10/"/>
    <id>http://yoursite.com/2016/09/02/Self-Paced-Learning-for-Latent-Variable-Models-NIPS10/</id>
    <published>2016-09-02T14:14:38.000Z</published>
    <updated>2016-09-03T00:51:58.971Z</updated>
    
    <content type="html"><![CDATA[<p>这一篇是Self-Paced Learning(SPL)的奠基之作。<br>SPL，固名思义，就是一步步，有自主步伐节奏得学。Motivation应该来自于09年Bengio提出的Curriculum Learning(CL)。CL受到认知科学的启发——人在学东西的时候也没办法一下子接受特别困难的知识，是从简单的开始学起。所以CL是根据某种先验，将按照困难度排好序的样本逐渐喂给模型。SPL与CL最大的不同之处在于这个排样本的先验是嵌入到模型里面的，是动态的，可以优化学习的。<br>这样子从易到难得学可以看成是一种正则化的手段，有助于加快收敛，并达到一个更好的local minimum.</p>
<a id="more"></a>
<p>考虑一个普通的机器学习优化问题<br>\begin{equation}<br>w_{t+1}=argmin_{w\in R^d}\left(r(w)+\sum_{i=1}^nf(x_i, y_i; w)\right)<br>\end{equation}<br>$r(.)$是一个正则化项，$f(.)$就是loss项了，比如负对数似然函数。SPL需要考虑样本的难易程度，以及一次要用多少样本。所以这篇文章引入了一个binary variable $v_i$，表示这个样本是否是简单的样本，只有简单的样本才在目标函数中有贡献。这样子新的目标函数是一个混合整数规划【mixed-integer program】问题：<br>\begin{equation}<br>(w_{t+1}, v_{t+1})=argmin_{w\in R^d, v\in {0,1}^n}\left(r(w)+\sum_{i=1}^nv_if(x_i, y_i; w) - \frac{1}{K}\sum_{i=1}{n}v_i\right)<br>\end{equation}<br>$K$是用来调整要选多少简单的样本：当$K$比较大时，为了使目标函数更小，那么就只有$f(.)$比较小【high likelihood，因为是负的】的样本会被选中。<br>更重要的是，每个样本是否是简单的其实不是单独考虑的——它们之间通过参数$w$联系起来，也即，一组样本是简单的条件是有$w$能够拟合这组样本，得到一个值比较小的$f(.)$。<br>在训练过程中，我们逐渐减小$K$的值，使得越来越多的样本能够被考虑进来。当$K$趋近于0时，优化(2)其实就相当于在优化(1)。<br>然而(2)这样的混合整数规划问题通常不好求解，所以我们松弛对参数$v$的约束：允许$v_i$的值落在[0, 1]之间。这个松弛是紧的【tight】，也就是说，取得最优值的$w$，对应的每个$v_i$一定是0或1。这很好理解：如果$f(x_i,y_i;w)\lt 1/K$，那么肯定$v_i=1$能取得最小值；如果$f(x_i,y_i;w)\gt 1/K$，那么肯定$v_i=0$能取得最小值。<br>经过这样的松弛之后，问题就比较好解了。特殊情况下，如果$r(.)$和$f(.)$都是凸的，那么这个目标函数就是个biconvex的目标函数，可以采用诸如坐标下降的方法求解。作者用的是alternative convex search(ACS)。一般情况下，如果$r(.)$和$f(.)$都是非凸的，也可以用类似的方法求解：给定$w$，最优的$v$值为$v_i=\delta(f(x_i, y_i; w)\lt 1/K$，其中$\delta(.)$为指示函数【indicator function】；给定$v$，优化(2)就跟优化(1)一样。<br>作者在实验中$w$的初值是设置所有$v_i=1$，然后用CCCP算法跑一段时间。</p>
<p>最后用孟德宇老师的一页ppt总结一下SPL：<br><img src="/uploads/SPL.png" alt="SPL"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一篇是Self-Paced Learning(SPL)的奠基之作。&lt;br&gt;SPL，固名思义，就是一步步，有自主步伐节奏得学。Motivation应该来自于09年Bengio提出的Curriculum Learning(CL)。CL受到认知科学的启发——人在学东西的时候也没办法一下子接受特别困难的知识，是从简单的开始学起。所以CL是根据某种先验，将按照困难度排好序的样本逐渐喂给模型。SPL与CL最大的不同之处在于这个排样本的先验是嵌入到模型里面的，是动态的，可以优化学习的。&lt;br&gt;这样子从易到难得学可以看成是一种正则化的手段，有助于加快收敛，并达到一个更好的local minimum.&lt;/p&gt;
    
    </summary>
    
      <category term="SelfPacedLearning" scheme="http://yoursite.com/categories/SelfPacedLearning/"/>
    
    
      <category term="NIPS" scheme="http://yoursite.com/tags/NIPS/"/>
    
      <category term="Latent" scheme="http://yoursite.com/tags/Latent/"/>
    
      <category term="SelfPacedLearning" scheme="http://yoursite.com/tags/SelfPacedLearning/"/>
    
  </entry>
  
  <entry>
    <title>Domain Adaptation from Multiple Sources via Auxiliary Classifiers_ICML09</title>
    <link href="http://yoursite.com/2016/09/01/Domain-Adaptation-from-Multiple-Sources-via-Auxiliary-Classifiers-ICML09/"/>
    <id>http://yoursite.com/2016/09/01/Domain-Adaptation-from-Multiple-Sources-via-Auxiliary-Classifiers-ICML09/</id>
    <published>2016-09-01T02:51:42.000Z</published>
    <updated>2016-09-01T03:10:24.971Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章做的是从多个源领域到单个目标领域的迁移，思想来源于Adaptive SVM[1]，就是使源领域模型参数“适应”到目标领域去。<br><a id="more"></a><br>假设现在有s个源领域$D^s=(x_i^s, y_i^s)|_{i=1}^{n_s}，s=1,2,…,P$，目标领域有标记数据$D_l^T=(x_i^T,y_i^T)|_{i=1}^{n_l}$和未标记数据$D_u^T=x_i^T|_{i=n_l+1}^{n_l+n_u}$，我们希望源领域的分类器$f^s(x)$和要学的目标领域分类器$f^T(x)$之间满足这样的关系：<br>\begin{equation}<br>f^T(x)=\sum_s\gamma_sf^s(x)+\Delta f(x)\\<br>s.t. \quad \sum_s\gamma_s=1<br>\end{equation}<br>扰动函数【perturbation function】$\Delta f(x)$用目标领域的标记数据$D_l^T$来学，根据[1]，<br>\begin{equation}<br>\Delta f(x)=\sum_{i=1}^{n_l}\alpha_i^Ty_i^Tk(x_i^T,x)<br>\end{equation}<br>类似地，<br>\begin{equation}<br>f^s(x)=\sum_s\gamma_s\sum_{i=1}^{n_s}\gamma_i^sy_i^sk(x_i^s,x)<br>\end{equation}<br>两者用相同的kernel<br>这样子，目标领域的分类器就变成了一些kernel的加权和<br>然而现在我们还没有考虑到怎么利用未标记的数据，根据manifold约束，在流形上相邻的特征它们的decision value也应该相近。因此作者认为，在domain adaption问题中，对于这些未标记的数据，目标分类器的decision value和与它比较相关的源领域分类器的decision value值也不应该差太远，所以作者提出了这样的一个data-dependent regularizer<br>\begin{equation}<br>\Omega_D(\textbf{f}_u^T)=\frac{1}{2}\sum_{i=n_l+1}^{n_T}\sum_s\gamma_s(f_i^T-f_i^s)^2=\frac{1}{2}\sum_s\gamma_s|\textbf{f}_u^T-\textbf{f}_u^s|^2<br>\end{equation}<br>$\gamma_s$表示目标领域与某个源领域之间的相关程度<br>$\textbf{f}_u^T=[f_{n_{l+1}}^T,…,f_{n_T}^T]’,\ \textbf{f}_u^s=[f_{n_{l+1}}^s, …, f_{n_T}^s]’$<br>所以最后的目标函数是<br>\begin{equation}<br>min_{f^T}\Omega(f^T)+\frac{1}{2}\sum_{i=1}^{n_l}(f_i^T-y_i^T)^2+\Omega_D(\textbf{f}_u^T)<br>\end{equation}<br>其中，$\Omega(f^T)$是目标分类器的参数正则化项<br>最后，作者还提出因为SVR【Support Vector Regression】通常能得到比较稀疏的解，所以还可以再加一个约束项<br>作者最后得出来的对偶形式也没有涉及太多的kernel运算，所以作者认为自己的算法比较scale</p>
<p>实验是在TRECVID 2005数据集上进行的</p>
<hr>
<p>[1] Crossdomain video concept detection using adaptive SVMs</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章做的是从多个源领域到单个目标领域的迁移，思想来源于Adaptive SVM[1]，就是使源领域模型参数“适应”到目标领域去。&lt;br&gt;
    
    </summary>
    
      <category term="TransferLearning" scheme="http://yoursite.com/categories/TransferLearning/"/>
    
    
      <category term="Transfer" scheme="http://yoursite.com/tags/Transfer/"/>
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
      <category term="ICML" scheme="http://yoursite.com/tags/ICML/"/>
    
  </entry>
  
  <entry>
    <title>Bilinear classifiers for visual recognition_NIPS09</title>
    <link href="http://yoursite.com/2016/08/31/Bilinear-classifiers-for-visual-recognition-NIPS09/"/>
    <id>http://yoursite.com/2016/08/31/Bilinear-classifiers-for-visual-recognition-NIPS09/</id>
    <published>2016-08-31T11:39:59.000Z</published>
    <updated>2016-09-01T03:03:19.187Z</updated>
    
    <content type="html"><![CDATA[<p>Bilinear模型作为linear模型的泛化版本，一般是为了提升模型的表达能力，但是这篇文章是反过来的：作者把参数$W$分解为一些低秩矩阵，也即“因子”【factor】的积，进而减少参数数量，防止过拟合。<br><a id="more"></a><br>具体得说，原本线性SVM的模型是<br>\begin{equation} \label{eq:1}<br>L(W)=\frac{1}{2}Tr(W^TW)+C\sum_nmax(0,1-y_nTr(W^TX_n)<br>\end{equation}<br>$X \in R^{n_y*n_x*n_f}$, $n_y$和$n_x$是空间维度，$n_f$是特征维度。为简化讨论，下面固定$n_f=1$<br>通过对W进行低秩分解【这里的秩的约束通过$W_x$和$W_y$的秩不会超过$d$来体现】<br>\begin{equation}<br>W=W_yW^T_x,\ where\ \ W_y\in R^{n_y*d}, W_x\in R^{n_x*d}<br>\end{equation}<br>这样子对$W$进行分解后的Bilinear SVM可以重新写成<br>\begin{equation}<br>L_(W_y, W_x)=\frac{1}{2}Tr(W_xW_y^TW_yW_x^T)+C\sum_nmax(0, 1-y_nTr(W_xW_y^TX_n))<br>\end{equation}<br>然而这个形式不好优化，我们得把它写成像公式$\ref{eq:1}$的形式，即<br>\begin{equation}<br>L(\tilde{W}_y, W_x)=\frac{1}{2}Tr(\tilde{W}_y^T\tilde{W}_y)+C\sum_nmax(0,1-y_nTr(\tilde{W}_y^T\tilde{X}_n))\\<br>where\ \tilde{W}_y=W_yA^{\frac{1}{2}}, \tilde{X}_n=X_nW_xA^{-\frac{1}{2}}, A=W_x^TW_x<br>\end{equation}<br>这是一个biconvex的目标函数，可以用coordinate descent来解<br>如果现在有$M$个任务，每个任务都有不同的数据，那么我们限定所有的任务共享相同的参数空间，即<br>\begin{equation}<br>L(W)=\frac{1}{2}\sum_mTr(W^{T}W)+\sum_mC_m\sum_nmax(0, 1-y_n^mTr(W^{T}X_n^m))<br>\end{equation}<br>这个模型还可以扩展成Multilinear的模型<br>其他的SVM模型，如Structural SVM，也可以写成这种形式</p>
<p></p><h2>实验</h2><br>作者使用了行人检测的数据集INRIA-MOTION[1]类的数据集UCF-Sports[2]，使用的是HOG特征和流特征flow feature[1]<p></p>
<p>[1] Human detection using oriented histograms of flow and appearance<br>[2] Action MACH a spatio-temporal Maximum Average Correlation Height filter for action recognition</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bilinear模型作为linear模型的泛化版本，一般是为了提升模型的表达能力，但是这篇文章是反过来的：作者把参数$W$分解为一些低秩矩阵，也即“因子”【factor】的积，进而减少参数数量，防止过拟合。&lt;br&gt;
    
    </summary>
    
      <category term="TransferLearning" scheme="http://yoursite.com/categories/TransferLearning/"/>
    
    
      <category term="Transfer" scheme="http://yoursite.com/tags/Transfer/"/>
    
      <category term="Bilinear" scheme="http://yoursite.com/tags/Bilinear/"/>
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
      <category term="NIPS" scheme="http://yoursite.com/tags/NIPS/"/>
    
  </entry>
  
</feed>
